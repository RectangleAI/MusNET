{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMLFSIK0xEoG",
        "outputId": "8e7bf145-d8da-441a-ab15-f438ba114f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.10.0.2)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WeZvkLV7xvMK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-6OUnWOjxw9a"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tENSm1HfyGvB"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/DataSetVoice/' # news_data or dict_data\n",
        "\n",
        "def tokenize(word): # create a tokenizer function\n",
        "    # word = word.replace('\\n', '')\n",
        "    return word.split(' ')\n",
        "\n",
        "# <sos>: start of a sequence; <eos>: end of a sequence.\n",
        "SRC = Field(tokenize=tokenize, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True,\n",
        "            include_lengths = True)\n",
        "\n",
        "TRG = Field(tokenize=tokenize, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=False,\n",
        "            include_lengths = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9G0vV4tZzSnR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_path+'/parallel_data_sample3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ-MNVIqznv8",
        "outputId": "3460d238-88f3-46e1-98de-671676a813ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NbfdHSTM1h66",
        "outputId": "944b2aec-40e9-4f02-d947-9cc09e08bfa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5b34104-1fd5-4038-a87d-9a45da7970e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Song</th>\n",
              "      <th>Guitar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>'C11' 'G4' 'D♯-1' 'C11' 'G5' 'D-1' 'C♯11' 'G4'...</td>\n",
              "      <td>'C11' 'D9' 'D-1' 'C♯11' 'D9' 'D-1' 'G♯9' 'D9' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>'C♯11' 'D7' 'D-1' 'C♯11' 'F7' 'D-1' 'G♯9' 'G4'...</td>\n",
              "      <td>'C11' 'C9' 'D♯-1' 'C11' 'C♯9' 'D♯-1' 'C11' 'D9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>'C♯11' 'D6' 'G-1' 'C♯11' 'G6' 'E-1' 'C♯11' 'B6...</td>\n",
              "      <td>'C11' 'G7' 'D-1' 'C♯11' 'G4' 'A-1' 'C♯11' 'G5'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>'G♯9' 'G4' 'C-1' 'G♯9' 'G5' 'C-1' 'G♯9' 'D6' '...</td>\n",
              "      <td>'C11' 'G5' 'A-1' 'C11' 'G6' 'D-1' 'C♯11' 'G4' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>'C♯11' 'G5' 'C♯0' 'C♯11' 'G6' 'D♯-1' 'G♯9' 'G4...</td>\n",
              "      <td>'G♯9' 'B6' 'C-1' 'A9' 'G4' 'C-1' 'A9' 'D6' 'C-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>3995</td>\n",
              "      <td>'G♯9' 'G5' 'C-1' 'G♯9' 'D6' 'C-1' 'G♯9' 'G6' '...</td>\n",
              "      <td>'A9' 'G4' 'C-1' 'C11' 'G4' 'D♯-1' 'C♯11' 'G4' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>3996</td>\n",
              "      <td>'A9' 'G5' 'C-1' 'A9' 'D6' 'C-1' 'A9' 'G6' 'C-1...</td>\n",
              "      <td>'C♯11' 'G4' 'D-1' 'G♯9' 'G4' 'C-1' 'A9' 'G4' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>3997</td>\n",
              "      <td>'C11' 'G6' 'E-1' 'C11' 'B6' 'D-1' 'C11' 'G7' '...</td>\n",
              "      <td>'A9' 'G5' 'C-1' 'C11' 'G4' 'B-1' 'C♯11' 'G4' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>3998</td>\n",
              "      <td>'C11' 'G5' 'F♯-1' 'C11' 'D6' 'D♯-1' 'C♯11' 'G4...</td>\n",
              "      <td>'C♯11' 'G4' 'D-1' 'G♯9' 'G4' 'C-1' 'A9' 'G4' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>3999</td>\n",
              "      <td>'C♯11' 'G5' 'E-1' 'G♯9' 'G4' 'C-1' 'G♯9' 'G5' ...</td>\n",
              "      <td>'A9' 'G5' 'C-1' 'C11' 'G4' 'F♯-1' 'C♯11' 'G4' ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5b34104-1fd5-4038-a87d-9a45da7970e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5b34104-1fd5-4038-a87d-9a45da7970e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5b34104-1fd5-4038-a87d-9a45da7970e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                             Guitar\n",
              "0              0  ...  'C11' 'D9' 'D-1' 'C♯11' 'D9' 'D-1' 'G♯9' 'D9' ...\n",
              "1              1  ...  'C11' 'C9' 'D♯-1' 'C11' 'C♯9' 'D♯-1' 'C11' 'D9...\n",
              "2              2  ...  'C11' 'G7' 'D-1' 'C♯11' 'G4' 'A-1' 'C♯11' 'G5'...\n",
              "3              3  ...  'C11' 'G5' 'A-1' 'C11' 'G6' 'D-1' 'C♯11' 'G4' ...\n",
              "4              4  ...  'G♯9' 'B6' 'C-1' 'A9' 'G4' 'C-1' 'A9' 'D6' 'C-...\n",
              "...          ...  ...                                                ...\n",
              "3995        3995  ...  'A9' 'G4' 'C-1' 'C11' 'G4' 'D♯-1' 'C♯11' 'G4' ...\n",
              "3996        3996  ...  'C♯11' 'G4' 'D-1' 'G♯9' 'G4' 'C-1' 'A9' 'G4' '...\n",
              "3997        3997  ...  'A9' 'G5' 'C-1' 'C11' 'G4' 'B-1' 'C♯11' 'G4' '...\n",
              "3998        3998  ...  'C♯11' 'G4' 'D-1' 'G♯9' 'G4' 'C-1' 'A9' 'G4' '...\n",
              "3999        3999  ...  'A9' 'G5' 'C-1' 'C11' 'G4' 'F♯-1' 'C♯11' 'G4' ...\n",
              "\n",
              "[4000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jISyzrQl1i6z"
      },
      "outputs": [],
      "source": [
        "df.drop(\"Unnamed: 0\",inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "APA4Q4ywzhpd"
      },
      "outputs": [],
      "source": [
        "train = df.iloc[0:3500]\n",
        "valid = df.iloc[3500:4000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fXUn73Eizwht"
      },
      "outputs": [],
      "source": [
        "train.to_csv(\"train.csv\",index=False)\n",
        "valid.to_csv(\"valid.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lM-Vne6PzEDu"
      },
      "outputs": [],
      "source": [
        "train_data = TabularDataset(\n",
        "           path = \"train.csv\", \n",
        "           format='csv',\n",
        "           skip_header=True, \n",
        "           fields=([(\"Song\", SRC), (\"Guitar\", TRG)]))\n",
        "\n",
        "valid_data = TabularDataset(\n",
        "           path = \"valid.csv\", \n",
        "           format='csv',\n",
        "           skip_header=True, \n",
        "           fields=([(\"Song\", SRC), (\"Guitar\", TRG)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NlT2wFNh0GiE"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq=1)\n",
        "TRG.build_vocab(train_data, min_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94VEgBwY0SL5",
        "outputId": "56535e8a-8cfa-4a71-8aa8-91065205fc0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(SRC.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JyOGxgJ0V08",
        "outputId": "185635b1-564f-4224-d1ca-4d06cafeb233"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(TRG.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrzW1FB30ZiN",
        "outputId": "393705f9-9d4c-4e6e-d7c2-f44d8019b93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 3500\n",
            "Number of testing examples: 500\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(valid_data.examples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O80m0sgY0kv8",
        "outputId": "a955d80e-c62f-4a22-f016-2285ebe74fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Song': [\"'c11'\", \"'g4'\", \"'d♯-1'\", \"'c11'\", \"'g5'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'d♯-1'\", \"'c♯11'\", \"'g5'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'f♯-1'\", \"'c11'\", \"'g5'\", \"'d-1'\", \"'c11'\", \"'d6'\", \"'d♯-1'\", \"'c♯11'\", \"'g4'\", \"'f♯-1'\", \"'c♯11'\", \"'g5'\", \"'d-1'\", \"'c♯11'\", \"'d6'\", \"'d♯-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯-1'\", \"'c11'\", \"'g5'\", \"'e-1'\", \"'c11'\", \"'d6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'g♯-1'\", \"'c♯11'\", \"'g5'\", \"'e-1'\", \"'c♯11'\", \"'d6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a♯-1'\", \"'c11'\", \"'g5'\", \"'e-1'\", \"'c11'\", \"'d6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'a♯-1'\", \"'c♯11'\", \"'g5'\", \"'e-1'\", \"'c♯11'\", \"'d6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'c0'\", \"'c11'\", \"'g5'\", \"'f♯-1'\", \"'c11'\", \"'d6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'c0'\", \"'c♯11'\", \"'g5'\", \"'f♯-1'\", \"'c♯11'\", \"'d6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'c♯0'\", \"'c11'\", \"'g5'\", \"'f-1'\", \"'c11'\", \"'d6'\", \"'f♯-1'\", \"'c11'\", \"'g6'\", \"'d♯-1'\", \"'c11'\", \"'b6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'c♯0'\", \"'c♯11'\", \"'g5'\", \"'f-1'\", \"'c♯11'\", \"'d6'\", \"'f♯-1'\", \"'c♯11'\", \"'g6'\", \"'d♯-1'\", \"'c♯11'\", \"'b6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'f♯0'\", \"'c11'\", \"'d6'\", \"'f♯-1'\", \"'c11'\", \"'g6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'f♯0'\", \"'c♯11'\", \"'d6'\", \"'f♯-1'\", \"'c♯11'\", \"'g6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'c♯0'\", \"'c11'\", \"'g5'\", \"'a♯-1'\", \"'c11'\", \"'d6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'c♯0'\", \"'c♯11'\", \"'g5'\", \"'a♯-1'\", \"'c♯11'\", \"'d6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'d♯0'\", \"'c11'\", \"'g5'\", \"'a-1'\", \"'c11'\", \"'d6'\", \"'f-1'\", \"'c♯11'\", \"'g4'\", \"'d♯0'\", \"'c♯11'\", \"'g5'\", \"'a-1'\", \"'c♯11'\", \"'d6'\", \"'f-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯0'\", \"'c11'\", \"'g5'\", \"'d♯-1'\", \"'c11'\", \"'d6'\", \"'a♯-1'\", \"'c11'\", \"'g6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'g♯0'\", \"'c♯11'\", \"'g5'\", \"'d♯-1'\", \"'c♯11'\", \"'d6'\", \"'a♯-1'\", \"'c♯11'\", \"'g6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'f♯0'\", \"'c11'\", \"'g5'\", \"'c0'\", \"'c11'\", \"'d6'\", \"'e-1'\", \"'c♯11'\", \"'g4'\", \"'f♯0'\", \"'c♯11'\", \"'g5'\", \"'c0'\", \"'c♯11'\", \"'d6'\", \"'e-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'d♯0'\", \"'c11'\", \"'g5'\", \"'d♯0'\", \"'c11'\", \"'d6'\", \"'a♯-1'\", \"'c11'\", \"'g6'\", \"'e-1'\", \"'c11'\", \"'b6'\", \"'d-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'d♯0'\", \"'c♯11'\", \"'g5'\", \"'d♯0'\", \"'c♯11'\", \"'d6'\", \"'a♯-1'\", \"'c♯11'\", \"'g6'\", \"'e-1'\", \"'c♯11'\", \"'b6'\", \"'d-1'\", \"'c♯11'\", \"'d7'\", \"'d-1'\", \"'c♯11'\", \"'f7'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'b-1'\", \"'c11'\", \"'g5'\", \"'e0'\", \"'c11'\", \"'d6'\", \"'g-1'\", \"'c11'\", \"'g6'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'b-1'\", \"'c♯11'\", \"'g5'\", \"'e0'\", \"'c♯11'\", \"'d6'\", \"'g-1'\", \"'c♯11'\", \"'g6'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯-1'\", \"'c11'\", \"'g5'\", \"'d0'\", \"'c11'\", \"'d6'\", \"'c1'\", \"'c11'\", \"'g6'\", \"'d♯-1'\", \"'c11'\", \"'b6'\", \"'d-1'\", \"'c11'\", \"'f♯8'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'g♯-1'\", \"'c♯11'\", \"'g5'\", \"'d0'\", \"'c♯11'\", \"'d6'\", \"'c1'\", \"'c♯11'\", \"'g6'\", \"'d♯-1'\", \"'c♯11'\", \"'b6'\", \"'d-1'\", \"'c♯11'\", \"'f♯8'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'f♯8'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'f♯8'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a-1'\", \"'c11'\", \"'g5'\", \"'d0'\", \"'c11'\", \"'d6'\", \"'e0'\", \"'c11'\", \"'g6'\", \"'e-1'\", \"'c11'\", \"'b6'\", \"'d-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'a-1'\", \"'c♯11'\", \"'g5'\", \"'d0'\", \"'c♯11'\", \"'d6'\", \"'e0'\", \"'c♯11'\", \"'g6'\", \"'e-1'\", \"'c♯11'\", \"'b6'\", \"'d-1'\", \"'c♯11'\", \"'d7'\", \"'d-1'\", \"'c♯11'\", \"'f7'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a-1'\", \"'c11'\", \"'g5'\", \"'d♯0'\", \"'c11'\", \"'d6'\", \"'d♯1'\", \"'c♯11'\", \"'g4'\", \"'a-1'\", \"'c♯11'\", \"'g5'\", \"'d♯0'\", \"'c♯11'\", \"'d6'\", \"'d♯1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'d♯0'\", \"'c11'\", \"'g5'\", \"'c0'\", \"'c11'\", \"'d6'\", \"'d0'\", \"'c11'\", \"'g6'\", \"'f♯-1'\", \"'c11'\", \"'b6'\", \"'e-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'d♯0'\", \"'c♯11'\", \"'g5'\", \"'c0'\", \"'c♯11'\", \"'d6'\", \"'d0'\", \"'c♯11'\", \"'g6'\", \"'f♯-1'\", \"'c♯11'\", \"'b6'\", \"'e-1'\"], 'Guitar': [\"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'C11'\", \"'B8'\", \"'D-1'\", \"'C11'\", \"'C9'\", \"'D-1'\", \"'C11'\", \"'D9'\", \"'F♯-1'\", \"'C11'\", \"'D♯9'\", \"'E-1'\", \"'C11'\", \"'E9'\", \"'D♯-1'\", \"'C♯11'\", \"'B8'\", \"'D-1'\", \"'C♯11'\", \"'C9'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'F♯-1'\", \"'C♯11'\", \"'D♯9'\", \"'E-1'\", \"'C♯11'\", \"'E9'\", \"'D♯-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'B8'\", \"'C-1'\", \"'G♯9'\", \"'C9'\", \"'C-1'\", \"'G♯9'\", \"'D♯9'\", \"'C-1'\", \"'G♯9'\", \"'E9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'B8'\", \"'C-1'\", \"'A9'\", \"'C9'\", \"'C-1'\", \"'A9'\", \"'D♯9'\", \"'C-1'\", \"'A9'\", \"'E9'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D♯-1'\", \"'C11'\", \"'D6'\", \"'D-1'\", \"'C11'\", \"'B8'\", \"'D-1'\", \"'C11'\", \"'C♯9'\", \"'D♯-1'\", \"'C11'\", \"'D9'\", \"'D♯-1'\", \"'C11'\", \"'D♯9'\", \"'D♯-1'\", \"'C11'\", \"'E9'\", \"'E-1'\", \"'C11'\", \"'F9'\", \"'D-1'\", \"'C11'\", \"'F♯9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D♯-1'\", \"'C♯11'\", \"'D6'\", \"'D-1'\", \"'C♯11'\", \"'B8'\", \"'D-1'\", \"'C♯11'\", \"'C♯9'\", \"'D♯-1'\", \"'C♯11'\", \"'D9'\", \"'D♯-1'\", \"'C♯11'\", \"'D♯9'\", \"'D♯-1'\", \"'C♯11'\", \"'E9'\", \"'E-1'\", \"'C♯11'\", \"'F9'\", \"'D-1'\", \"'C♯11'\", \"'F♯9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'B8'\", \"'C-1'\", \"'G♯9'\", \"'D♯9'\", \"'C-1'\", \"'G♯9'\", \"'E9'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'G♯9'\", \"'C♯9'\", \"'C-1'\", \"'G♯9'\", \"'F9'\", \"'C-1'\", \"'G♯9'\", \"'F♯9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'B8'\", \"'C-1'\", \"'A9'\", \"'D♯9'\", \"'C-1'\", \"'A9'\", \"'E9'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'C♯9'\", \"'C-1'\", \"'A9'\", \"'F9'\", \"'C-1'\", \"'A9'\", \"'F♯9'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'E-1'\", \"'C11'\", \"'G5'\", \"'D♯-1'\", \"'C11'\", \"'A8'\", \"'D-1'\", \"'C11'\", \"'A♯8'\", \"'D♯-1'\", \"'C11'\", \"'C♯9'\", \"'F-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C11'\", \"'D♯9'\", \"'F-1'\", \"'C11'\", \"'E9'\", \"'G♯-1'\", \"'C11'\", \"'F♯9'\", \"'D-1'\", \"'C11'\", \"'G9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'E-1'\", \"'C♯11'\", \"'G5'\", \"'D♯-1'\", \"'C♯11'\", \"'A8'\", \"'D-1'\", \"'C♯11'\", \"'A♯8'\", \"'D♯-1'\", \"'C♯11'\", \"'C♯9'\", \"'F-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D♯9'\", \"'F-1'\", \"'C♯11'\", \"'E9'\", \"'G♯-1'\", \"'C♯11'\", \"'F♯9'\", \"'D-1'\", \"'C♯11'\", \"'G9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'D♯9'\", \"'C-1'\", \"'G♯9'\", \"'E9'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'C♯9'\", \"'C-1'\", \"'G♯9'\", \"'F♯9'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'A8'\", \"'C-1'\", \"'G♯9'\", \"'A♯8'\", \"'C-1'\", \"'G♯9'\", \"'G9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D♯9'\", \"'C-1'\", \"'A9'\", \"'E9'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'C♯9'\", \"'C-1'\", \"'A9'\", \"'F♯9'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'A8'\", \"'C-1'\", \"'A9'\", \"'A♯8'\", \"'C-1'\", \"'A9'\", \"'G9'\", \"'C-1'\", \"'C11'\", \"'G5'\", \"'E-1'\", \"'C11'\", \"'D6'\", \"'D♯-1'\", \"'C11'\", \"'B8'\", \"'D-1'\", \"'C11'\", \"'D9'\", \"'D♯-1'\", \"'C11'\", \"'E9'\", \"'D-1'\", \"'C♯11'\", \"'G5'\", \"'E-1'\", \"'C♯11'\", \"'D6'\", \"'D♯-1'\", \"'C♯11'\", \"'B8'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D♯-1'\", \"'C♯11'\", \"'E9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'B8'\", \"'C-1'\", \"'G♯9'\", \"'E9'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'B8'\", \"'C-1'\", \"'A9'\", \"'E9'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'C11'\", \"'G5'\", \"'D♯-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'G5'\", \"'D♯-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'F-1'\", \"'C♯11'\", \"'G4'\", \"'F-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C11'\", \"'D6'\", \"'D-1'\", \"'C11'\", \"'A8'\", \"'D♯-1'\", \"'C11'\", \"'A♯8'\", \"'D♯-1'\", \"'C11'\", \"'B8'\", \"'D♯-1'\", \"'C11'\", \"'C9'\", \"'F♯-1'\", \"'C11'\", \"'C♯9'\", \"'A♯-1'\", \"'C11'\", \"'D9'\", \"'G♯-1'\", \"'C11'\", \"'D♯9'\", \"'G-1'\", \"'C11'\", \"'E9'\", \"'A-1'\", \"'C11'\", \"'F9'\", \"'E-1'\", \"'C11'\", \"'F♯9'\", \"'D-1'\", \"'C11'\", \"'G9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'D6'\", \"'D-1'\", \"'C♯11'\", \"'A8'\", \"'D♯-1'\", \"'C♯11'\", \"'A♯8'\", \"'D♯-1'\", \"'C♯11'\", \"'B8'\", \"'D♯-1'\", \"'C♯11'\", \"'C9'\", \"'F♯-1'\", \"'C♯11'\", \"'C♯9'\", \"'A♯-1'\", \"'C♯11'\", \"'D9'\", \"'G♯-1'\", \"'C♯11'\", \"'D♯9'\", \"'G-1'\", \"'C♯11'\", \"'E9'\", \"'A-1'\", \"'C♯11'\", \"'F9'\", \"'E-1'\", \"'C♯11'\", \"'F♯9'\", \"'D-1'\", \"'C♯11'\", \"'G9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'B8'\", \"'C-1'\", \"'G♯9'\", \"'C9'\", \"'C-1'\", \"'G♯9'\", \"'D♯9'\", \"'C-1'\", \"'G♯9'\", \"'E9'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'G♯9'\", \"'C♯9'\", \"'C-1'\", \"'G♯9'\", \"'F9'\", \"'C-1'\", \"'G♯9'\", \"'F♯9'\", \"'C-1'\", \"'G♯9'\", \"'A8'\", \"'C-1'\", \"'G♯9'\", \"'A♯8'\", \"'C-1'\", \"'G♯9'\", \"'G9'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'B8'\", \"'C-1'\", \"'A9'\", \"'C9'\", \"'C-1'\", \"'A9'\", \"'D♯9'\", \"'C-1'\", \"'A9'\", \"'E9'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'C♯9'\", \"'C-1'\", \"'A9'\", \"'F9'\", \"'C-1'\", \"'A9'\", \"'F♯9'\", \"'C-1'\", \"'A9'\", \"'A8'\", \"'C-1'\", \"'A9'\", \"'A♯8'\", \"'C-1'\", \"'A9'\", \"'G9'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'E-1'\", \"'C11'\", \"'G5'\", \"'F-1'\", \"'C11'\", \"'B8'\", \"'D-1'\", \"'C11'\", \"'C♯9'\", \"'D♯-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C11'\", \"'F9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'E-1'\", \"'C♯11'\", \"'G5'\", \"'F-1'\", \"'C♯11'\", \"'B8'\", \"'D-1'\", \"'C♯11'\", \"'C♯9'\", \"'D♯-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'F9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'B8'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'C♯9'\", \"'C-1'\", \"'G♯9'\", \"'F9'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'B8'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'C♯9'\", \"'C-1'\", \"'A9'\", \"'F9'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C11'\", \"'G5'\", \"'G-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'G5'\", \"'G-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'G-1'\", \"'C11'\", \"'G5'\", \"'G♯-1'\", \"'C11'\", \"'D6'\", \"'D-1'\", \"'C11'\", \"'D9'\", \"'D-1'\", \"'C11'\", \"'D♯9'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'G-1'\", \"'C♯11'\", \"'G5'\", \"'G♯-1'\", \"'C♯11'\", \"'D6'\", \"'D-1'\", \"'C♯11'\", \"'D9'\", \"'D-1'\", \"'C♯11'\", \"'D♯9'\", \"'D-1'\", \"'G♯9'\", \"'D9'\", \"'C-1'\", \"'G♯9'\", \"'D♯9'\", \"'C-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D9'\", \"'C-1'\", \"'A9'\", \"'D♯9'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'F♯-1'\", \"'C11'\", \"'G5'\", \"'G♯-1'\", \"'C11'\", \"'D6'\", \"'D-1'\"]}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiQ52zvI13rL",
        "outputId": "0ca12e63-6f4a-4a9e-9f3e-52ff02d544d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = False,\n",
        "     sort_key = lambda x : len(x.Song),\n",
        "     device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6Ltsu2XK19it"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)    \n",
        "    def forward(self, src, src_len):        \n",
        "        embedded = self.dropout(self.embedding(src))                \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len,enforce_sorted=False)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jmNP3Pv-2AV6"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        return F.softmax(attention, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IUDBddN12DJw"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # print(output.shape)\n",
        "        # print(hidden.shape)\n",
        "        # assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jiLElypA2HEX"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device, teacher_forcing_ratio = 0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio \n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg,):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len.cpu())\n",
        "        input = trg[0,:]\n",
        "\n",
        "        mask = self.create_mask(src)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
        "            top1 = output.argmax(1) \n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Zha16G3P2Iv9"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3clH86SK2KwR",
        "outputId": "002f0377-5b03-4001-ca82-296eed9b9f2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(107, 128)\n",
              "    (rnn): GRU(128, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(136, 128)\n",
              "    (rnn): GRU(640, 256)\n",
              "    (fc_out): Linear(in_features=896, out_features=136, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnMHTw_L2M8_",
        "outputId": "de51989f-7272-4940-8109-d301cac3e052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,764,104 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "W5d9guKL2Puq"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.003  # 0.003 in paper\n",
        "patience = 0\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer=optimizer,\n",
        "                mode='min', factor=0.9, # 0.9 in paper\n",
        "                patience=patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WM4Lpp552TF7"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wwugrPyJ2ax_"
      },
      "outputs": [],
      "source": [
        "def update(epoch, valid_loss, valid_acc, \n",
        "           best_valid_loss, best_valid_acc, acc_valid_loss,\n",
        "           update_type='acc'):\n",
        "    global best_valid_epoch, early_stop_patience, full_patience, best_train_step, train_steps, exp_num\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"[Epoch: {}][Validatiing...]\".format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        print('\\t\\t Better Valid Loss!')\n",
        "        best_valid_loss = valid_loss\n",
        "        if update_type == 'loss':\n",
        "            torch.save(model.state_dict(), 'loss-model.pt')\n",
        "        early_stop_patience = full_patience  # restore full patience if obtain new minimum of the loss\n",
        "    else:\n",
        "        if early_stop_patience > 0:\n",
        "            early_stop_patience += -1\n",
        "    \n",
        "    if valid_acc > best_valid_acc or (valid_acc == best_valid_acc and valid_loss < acc_valid_loss):\n",
        "        print('\\t\\t Better Valid Acc!')\n",
        "        best_valid_acc = valid_acc\n",
        "        acc_valid_loss = valid_loss\n",
        "        best_valid_epoch = epoch\n",
        "        best_train_step = train_steps\n",
        "        if update_type == 'acc':\n",
        "            torch.save(model.state_dict(), 'experiments/exp' + str(exp_num) + '/acc-model-seq2seq.pt')\n",
        "    print(f'\\t patience: {early_stop_patience}/{full_patience}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    print(f'\\t BEST. Val. Loss: {best_valid_loss:.3f} | BEST. Val. Acc: {best_valid_acc:.3f} | Val. Loss: {acc_valid_loss:.3f} | BEST. Val. Epoch: {best_valid_epoch} | BEST. Val. Step: {best_train_step}')\n",
        "    print(\"---------------------------------------\\n\")\n",
        "    return best_valid_loss, best_valid_acc, acc_valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uEQptv5E2iNk"
      },
      "outputs": [],
      "source": [
        "n_examples = len(train_data.examples)\n",
        "\n",
        "def train(model, iterator, \n",
        "          optimizer, criterion, \n",
        "          clip, epoch,\n",
        "          scheduler, valid_iterator):\n",
        "    \n",
        "    global best_valid_loss, acc_valid_loss, best_valid_acc, best_valid_epoch, train_steps, report_steps, tfr\n",
        "    model.train()\n",
        "    model.teacher_forcing_ratio = tfr\n",
        "    print(\"[Train]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.Song\n",
        "        trg, trg_len = batch.Guitar\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        running_loss = epoch_loss / (i + 1)\n",
        "        \n",
        "        # print every 50 batches (50 steps)\n",
        "        if i % report_steps == report_steps - 1:\n",
        "            train_steps += report_steps  # by doing so, the last batch is neglected\n",
        "            for param_group in optimizer.param_groups:\n",
        "                lr = param_group['lr']\n",
        "            print('[Epoch: {}][#examples: {}/{}][#steps: {}]'.format(epoch, (i+1) * BATCH_SIZE, n_examples, train_steps))\n",
        "            print(f'\\tTrain Loss: {running_loss:.3f} | Train PPL: {math.exp(running_loss):7.3f} | lr: {lr:.3e}')\n",
        "            \n",
        "            # eval the validation set for every * steps\n",
        "            if (train_steps % (10 * report_steps)) == 0:\n",
        "                print('-----val------')\n",
        "                valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler)\n",
        "                print('-----tst------')\n",
        "                # test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "                best_valid_loss, best_valid_acc, acc_valid_loss = update(epoch, valid_loss, valid_acc, \n",
        "                                                         best_valid_loss, best_valid_acc, acc_valid_loss,\n",
        "                                                         update_type='loss')\n",
        "                scheduler.step(valid_loss)  # must be placed here otherwise the test acc messes up\n",
        "                model.train()\n",
        "                \n",
        "            \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JTvJHDWE2j22"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, scheduler, is_test=False):\n",
        "    \n",
        "    model.eval()\n",
        "    model.teacher_forcing_ratio = 0 #  turn off teacher forcing\n",
        "    print(\"[Eval Start]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    global valid_data, test_data, tfr\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.Song\n",
        "            trg, trg_len = batch.Guitar\n",
        "\n",
        "            output = model(src, src_len, trg)\n",
        "            \n",
        "            # ---------compute acc START----------\n",
        "            pred = output[1:].argmax(2).permute(1, 0) # [batch_size, trg_len]\n",
        "            ref = trg[1:].permute(1, 0)\n",
        "            # consider the last batch as well\n",
        "            size = pred.shape[0]\n",
        "            for j in range(size):\n",
        "                \n",
        "                pred_j = pred[j, :]\n",
        "                pred_j_toks = []\n",
        "                for t in pred_j:\n",
        "                    tok = TRG.vocab.itos[t]\n",
        "                    if tok == '<eos>':\n",
        "                        break\n",
        "                    else:\n",
        "                        pred_j_toks.append(tok)\n",
        "                pred_j = ''.join(pred_j_toks)\n",
        "                \n",
        "                ref_j = ref[j, :]\n",
        "                ref_j_toks = []\n",
        "                for t in ref_j:\n",
        "                    tok = TRG.vocab.itos[t]\n",
        "                    if tok == '<eos>':\n",
        "                        break\n",
        "                    else:\n",
        "                        ref_j_toks.append(tok)\n",
        "                ref_j = ''.join(ref_j_toks)\n",
        "                \n",
        "                if pred_j == ref_j:\n",
        "                    correct += 1\n",
        "            # ---------compute acc END----------\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        # compute loss and acc\n",
        "        epoch_loss = epoch_loss / len(iterator)\n",
        "        # sheduler applies on acc\n",
        "        if not is_test:\n",
        "            acc = correct / len(valid_data.examples)\n",
        "            \n",
        "        else:\n",
        "            acc = correct / len(test_data.examples)\n",
        "        \n",
        "        print('The number of correct predictions: {}'.format(correct))\n",
        "        \n",
        "        model.teacher_forcing_ratio = tfr  # restore teacher-forcing ratio\n",
        "        print(\"[Eval End]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    return epoch_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GG8sqkiQ2mo9"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H-yBw8h2oXj",
        "outputId": "087c9457-cad2-4d17-b94c-19fa494cda08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train]: Current Teacher Forcing Ratio: 0.800\n",
            "[Epoch: 0][#examples: 10/3500][#steps: 10]\n",
            "\tTrain Loss: 3.822 | Train PPL:  45.716 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 20/3500][#steps: 20]\n",
            "\tTrain Loss: 3.500 | Train PPL:  33.105 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 30/3500][#steps: 30]\n",
            "\tTrain Loss: 3.253 | Train PPL:  25.863 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 40/3500][#steps: 40]\n",
            "\tTrain Loss: 3.005 | Train PPL:  20.179 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 50/3500][#steps: 50]\n",
            "\tTrain Loss: 2.782 | Train PPL:  16.144 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 60/3500][#steps: 60]\n",
            "\tTrain Loss: 2.631 | Train PPL:  13.883 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 70/3500][#steps: 70]\n",
            "\tTrain Loss: 2.517 | Train PPL:  12.389 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 80/3500][#steps: 80]\n",
            "\tTrain Loss: 2.438 | Train PPL:  11.446 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 90/3500][#steps: 90]\n",
            "\tTrain Loss: 2.369 | Train PPL:  10.692 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 100/3500][#steps: 100]\n",
            "\tTrain Loss: 2.320 | Train PPL:  10.179 | lr: 3.000e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 0\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Loss!\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 20/20\n",
            "\t Val. Loss: 1.912 | Val. Acc: 0.000 | Val. PPL:   6.770\n",
            "\t BEST. Val. Loss: 1.912 | BEST. Val. Acc: 0.000 | Val. Loss: 1.912 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 110/3500][#steps: 110]\n",
            "\tTrain Loss: 2.284 | Train PPL:   9.816 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 120/3500][#steps: 120]\n",
            "\tTrain Loss: 2.235 | Train PPL:   9.351 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 130/3500][#steps: 130]\n",
            "\tTrain Loss: 2.205 | Train PPL:   9.069 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 140/3500][#steps: 140]\n",
            "\tTrain Loss: 2.183 | Train PPL:   8.877 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 150/3500][#steps: 150]\n",
            "\tTrain Loss: 2.162 | Train PPL:   8.692 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 160/3500][#steps: 160]\n",
            "\tTrain Loss: 2.132 | Train PPL:   8.430 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 170/3500][#steps: 170]\n",
            "\tTrain Loss: 2.110 | Train PPL:   8.245 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 180/3500][#steps: 180]\n",
            "\tTrain Loss: 2.078 | Train PPL:   7.989 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 190/3500][#steps: 190]\n",
            "\tTrain Loss: 2.041 | Train PPL:   7.698 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 200/3500][#steps: 200]\n",
            "\tTrain Loss: 2.019 | Train PPL:   7.529 | lr: 3.000e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 0\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 19/20\n",
            "\t Val. Loss: 2.656 | Val. Acc: 0.000 | Val. PPL:  14.244\n",
            "\t BEST. Val. Loss: 1.912 | BEST. Val. Acc: 0.000 | Val. Loss: 1.912 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 210/3500][#steps: 210]\n",
            "\tTrain Loss: 1.993 | Train PPL:   7.336 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 220/3500][#steps: 220]\n",
            "\tTrain Loss: 1.975 | Train PPL:   7.207 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 230/3500][#steps: 230]\n",
            "\tTrain Loss: 1.956 | Train PPL:   7.069 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 240/3500][#steps: 240]\n",
            "\tTrain Loss: 1.939 | Train PPL:   6.949 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 250/3500][#steps: 250]\n",
            "\tTrain Loss: 1.923 | Train PPL:   6.841 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 260/3500][#steps: 260]\n",
            "\tTrain Loss: 1.905 | Train PPL:   6.722 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 270/3500][#steps: 270]\n",
            "\tTrain Loss: 1.884 | Train PPL:   6.582 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 280/3500][#steps: 280]\n",
            "\tTrain Loss: 1.870 | Train PPL:   6.488 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 290/3500][#steps: 290]\n",
            "\tTrain Loss: 1.855 | Train PPL:   6.391 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 300/3500][#steps: 300]\n",
            "\tTrain Loss: 1.843 | Train PPL:   6.313 | lr: 2.700e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 0\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 18/20\n",
            "\t Val. Loss: 2.678 | Val. Acc: 0.000 | Val. PPL:  14.561\n",
            "\t BEST. Val. Loss: 1.912 | BEST. Val. Acc: 0.000 | Val. Loss: 1.912 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 310/3500][#steps: 310]\n",
            "\tTrain Loss: 1.830 | Train PPL:   6.233 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 320/3500][#steps: 320]\n",
            "\tTrain Loss: 1.812 | Train PPL:   6.124 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 330/3500][#steps: 330]\n",
            "\tTrain Loss: 1.797 | Train PPL:   6.033 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 340/3500][#steps: 340]\n",
            "\tTrain Loss: 1.781 | Train PPL:   5.936 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 350/3500][#steps: 350]\n",
            "\tTrain Loss: 1.767 | Train PPL:   5.852 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 360/3500][#steps: 360]\n",
            "\tTrain Loss: 1.751 | Train PPL:   5.759 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 370/3500][#steps: 370]\n",
            "\tTrain Loss: 1.734 | Train PPL:   5.664 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 380/3500][#steps: 380]\n",
            "\tTrain Loss: 1.717 | Train PPL:   5.566 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 390/3500][#steps: 390]\n",
            "\tTrain Loss: 1.698 | Train PPL:   5.466 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 400/3500][#steps: 400]\n",
            "\tTrain Loss: 1.691 | Train PPL:   5.426 | lr: 2.430e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "Exiting loop\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 1\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "acc_valid_loss = float('inf')\n",
        "best_valid_acc = float(-1)\n",
        "best_valid_epoch = -1\n",
        "best_train_step = -1\n",
        "full_patience = 20\n",
        "early_stop_patience = full_patience\n",
        "train_steps = 0\n",
        "report_steps = 10\n",
        "exp_num = 1\n",
        "\n",
        "try:\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        if epoch <= 15:\n",
        "            early_stop_patience = full_patience\n",
        "\n",
        "        if early_stop_patience == 0:\n",
        "            print(\"Early Stopping!\")\n",
        "            # break\n",
        "            # abandon early stopping because we found best epoch in a long run\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        tfr = max(1 - (float(10 + epoch * 1.5) / 50), 0.2) \n",
        "\n",
        "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP, epoch, scheduler, valid_iterator)\n",
        "\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler, is_test=False)\n",
        "        #test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        best_valid_loss, best_valid_acc, acc_valid_loss = update(epoch, valid_loss, valid_acc, \n",
        "                                                 best_valid_loss, best_valid_acc, acc_valid_loss, update_type='loss')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        # print(f'\\t Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test ACC: {test_acc:.3f}')\n",
        "except KeyboardInterrupt:\n",
        "        print(\"Exiting loop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8UmBClVIjGLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e49934-46ae-4356-e878-54337be4cb97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 0\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "| Valid Loss: 1.912 | Valid PPL:   6.770 | Valid ACC: 0.000\n"
          ]
        }
      ],
      "source": [
        "exp_num = 1\n",
        "# model.load_state_dict(torch.load('experiments/exp' + str(exp_num) + '/acc-model-seq2seq.pt'))\n",
        "model.load_state_dict(torch.load('loss-model.pt'))\n",
        "\n",
        "\n",
        "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler, is_test=False)\n",
        "# test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "\n",
        "# Note that the final translation accs might differ from below because of floating point error.\n",
        "# But they should be the same in most of the cases.\n",
        "print(f'| Valid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f} | Valid ACC: {valid_acc:.3f}')\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test ACC: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    print(trg_tokens)\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "metadata": {
        "id": "lUJOGGLs-KU4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 1\n",
        "\n",
        "src = vars(valid_data.examples[example_idx])['Song']\n",
        "trg = vars(valid_data.examples[example_idx])['Guitar']\n",
        "\n",
        "print(f'Song = {src}')\n",
        "print(f'Guitar = {trg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iokg_vQT-M6K",
        "outputId": "3ba3c57a-98b8-47ef-aa0c-018ef6939d07"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Song = [\"'c11'\", \"'g5'\", \"'a♯-1'\", \"'c11'\", \"'d6'\", \"'c♯0'\", \"'c11'\", \"'g6'\", \"'d♯0'\", \"'c11'\", \"'b6'\", \"'f♯-1'\", \"'c11'\", \"'d7'\", \"'f-1'\", \"'c11'\", \"'f7'\", \"'f-1'\", \"'c11'\", \"'g7'\", \"'f-1'\", \"'c11'\", \"'a7'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'d0'\", \"'c♯11'\", \"'g5'\", \"'a♯-1'\", \"'c♯11'\", \"'d6'\", \"'c♯0'\", \"'c♯11'\", \"'g6'\", \"'d♯0'\", \"'c♯11'\", \"'b6'\", \"'f♯-1'\", \"'c♯11'\", \"'d7'\", \"'f-1'\", \"'c♯11'\", \"'f7'\", \"'f-1'\", \"'c♯11'\", \"'g7'\", \"'f-1'\", \"'c♯11'\", \"'a7'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'g7'\", \"'c-1'\", \"'g♯9'\", \"'a7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g7'\", \"'c-1'\", \"'a9'\", \"'a7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯-1'\", \"'c11'\", \"'g5'\", \"'c0'\", \"'c11'\", \"'d6'\", \"'b-1'\", \"'c11'\", \"'g6'\", \"'e0'\", \"'c11'\", \"'b6'\", \"'f-1'\", \"'c11'\", \"'d7'\", \"'d♯-1'\", \"'c11'\", \"'f7'\", \"'d♯-1'\", \"'c11'\", \"'g7'\", \"'e-1'\", \"'c11'\", \"'a7'\", \"'d♯-1'\", \"'c♯11'\", \"'g4'\", \"'g♯-1'\", \"'c♯11'\", \"'g5'\", \"'c0'\", \"'c♯11'\", \"'d6'\", \"'b-1'\", \"'c♯11'\", \"'g6'\", \"'e0'\", \"'c♯11'\", \"'b6'\", \"'f-1'\", \"'c♯11'\", \"'d7'\", \"'d♯-1'\", \"'c♯11'\", \"'f7'\", \"'d♯-1'\", \"'c♯11'\", \"'g7'\", \"'e-1'\", \"'c♯11'\", \"'a7'\", \"'d♯-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'g7'\", \"'c-1'\", \"'g♯9'\", \"'a7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g7'\", \"'c-1'\", \"'a9'\", \"'a7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'f-1'\", \"'c11'\", \"'g5'\", \"'c♯0'\", \"'c11'\", \"'d6'\", \"'g♯-1'\", \"'c11'\", \"'g6'\", \"'e0'\", \"'c11'\", \"'b6'\", \"'e-1'\", \"'c11'\", \"'d7'\", \"'d♯-1'\", \"'c11'\", \"'f7'\", \"'e-1'\", \"'c11'\", \"'a7'\", \"'d♯-1'\", \"'c♯11'\", \"'g4'\", \"'f-1'\", \"'c♯11'\", \"'g5'\", \"'c♯0'\", \"'c♯11'\", \"'d6'\", \"'g♯-1'\", \"'c♯11'\", \"'g6'\", \"'e0'\", \"'c♯11'\", \"'b6'\", \"'e-1'\", \"'c♯11'\", \"'d7'\", \"'d♯-1'\", \"'c♯11'\", \"'f7'\", \"'e-1'\", \"'c♯11'\", \"'a7'\", \"'d♯-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'a7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'a7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a-1'\", \"'c11'\", \"'g5'\", \"'a♯-1'\", \"'c11'\", \"'d6'\", \"'c0'\", \"'c11'\", \"'g6'\", \"'d♯0'\", \"'c11'\", \"'b6'\", \"'g-1'\", \"'c11'\", \"'d7'\", \"'e-1'\", \"'c11'\", \"'f7'\", \"'d♯-1'\", \"'c11'\", \"'g7'\", \"'d♯-1'\", \"'c11'\", \"'a7'\", \"'d♯-1'\", \"'c11'\", \"'c♯8'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'a-1'\", \"'c♯11'\", \"'g5'\", \"'a♯-1'\", \"'c♯11'\", \"'d6'\", \"'c0'\", \"'c♯11'\", \"'g6'\", \"'d♯0'\", \"'c♯11'\", \"'b6'\", \"'g-1'\", \"'c♯11'\", \"'d7'\", \"'e-1'\", \"'c♯11'\", \"'f7'\", \"'d♯-1'\", \"'c♯11'\", \"'g7'\", \"'d♯-1'\", \"'c♯11'\", \"'a7'\", \"'d♯-1'\", \"'c♯11'\", \"'c♯8'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'g7'\", \"'c-1'\", \"'g♯9'\", \"'a7'\", \"'c-1'\", \"'g♯9'\", \"'c♯8'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g7'\", \"'c-1'\", \"'a9'\", \"'a7'\", \"'c-1'\", \"'a9'\", \"'c♯8'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a♯-1'\", \"'c11'\", \"'g5'\", \"'f♯-1'\", \"'c11'\", \"'d6'\", \"'f0'\", \"'c11'\", \"'g6'\", \"'c0'\", \"'c11'\", \"'b6'\", \"'f♯-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'g7'\", \"'d♯-1'\", \"'c♯11'\", \"'g4'\", \"'a♯-1'\", \"'c♯11'\", \"'g5'\", \"'f♯-1'\", \"'c♯11'\", \"'d6'\", \"'f0'\", \"'c♯11'\", \"'g6'\", \"'c0'\", \"'c♯11'\", \"'b6'\", \"'f♯-1'\", \"'c♯11'\", \"'d7'\", \"'d-1'\", \"'c♯11'\", \"'g7'\", \"'d♯-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'g7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'g7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'a♯-1'\", \"'c11'\", \"'g5'\", \"'f♯-1'\", \"'c11'\", \"'d6'\", \"'g♯0'\", \"'c11'\", \"'g6'\", \"'d0'\", \"'c11'\", \"'b6'\", \"'f-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c11'\", \"'b7'\", \"'d-1'\", \"'c11'\", \"'c♯8'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'a♯-1'\", \"'c♯11'\", \"'g5'\", \"'f♯-1'\", \"'c♯11'\", \"'d6'\", \"'g♯0'\", \"'c♯11'\", \"'g6'\", \"'d0'\", \"'c♯11'\", \"'b6'\", \"'f-1'\", \"'c♯11'\", \"'d7'\", \"'d-1'\", \"'c♯11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'b7'\", \"'d-1'\", \"'c♯11'\", \"'c♯8'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'b7'\", \"'c-1'\", \"'g♯9'\", \"'c♯8'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'b7'\", \"'c-1'\", \"'a9'\", \"'c♯8'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'f0'\", \"'c11'\", \"'g5'\", \"'c♯0'\", \"'c11'\", \"'d6'\", \"'b-1'\", \"'c11'\", \"'g6'\", \"'g0'\", \"'c11'\", \"'b6'\", \"'f-1'\", \"'c11'\", \"'d7'\", \"'d-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c11'\", \"'b7'\", \"'d-1'\", \"'c11'\", \"'c♯8'\", \"'d-1'\", \"'c11'\", \"'d8'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'f0'\", \"'c♯11'\", \"'g5'\", \"'c♯0'\", \"'c♯11'\", \"'d6'\", \"'b-1'\", \"'c♯11'\", \"'g6'\", \"'g0'\", \"'c♯11'\", \"'b6'\", \"'f-1'\", \"'c♯11'\", \"'d7'\", \"'d-1'\", \"'c♯11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'b7'\", \"'d-1'\", \"'c♯11'\", \"'c♯8'\", \"'d-1'\", \"'c♯11'\", \"'d8'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'b7'\", \"'c-1'\", \"'g♯9'\", \"'c♯8'\", \"'c-1'\", \"'g♯9'\", \"'d8'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'b7'\", \"'c-1'\", \"'a9'\", \"'c♯8'\", \"'c-1'\", \"'a9'\", \"'d8'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯-1'\", \"'c11'\", \"'g5'\", \"'g♯0'\", \"'c11'\", \"'d6'\", \"'f♯-1'\", \"'c11'\", \"'g6'\", \"'a♯0'\", \"'c11'\", \"'b6'\", \"'d♯-1'\", \"'c11'\", \"'d7'\", \"'e-1'\", \"'c11'\", \"'f7'\", \"'e-1'\", \"'c11'\", \"'g7'\", \"'d-1'\", \"'c11'\", \"'b7'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'g♯-1'\", \"'c♯11'\", \"'g5'\", \"'g♯0'\", \"'c♯11'\", \"'d6'\", \"'f♯-1'\", \"'c♯11'\", \"'g6'\", \"'a♯0'\", \"'c♯11'\", \"'b6'\", \"'d♯-1'\", \"'c♯11'\", \"'d7'\", \"'e-1'\", \"'c♯11'\", \"'f7'\", \"'e-1'\", \"'c♯11'\", \"'g7'\", \"'d-1'\", \"'c♯11'\", \"'b7'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\", \"'g♯9'\", \"'g6'\", \"'c-1'\", \"'g♯9'\", \"'b6'\", \"'c-1'\", \"'g♯9'\", \"'d7'\", \"'c-1'\", \"'g♯9'\", \"'f7'\", \"'c-1'\", \"'g♯9'\", \"'g7'\", \"'c-1'\", \"'g♯9'\", \"'b7'\", \"'c-1'\", \"'a9'\", \"'g4'\", \"'c-1'\", \"'a9'\", \"'g5'\", \"'c-1'\", \"'a9'\", \"'d6'\", \"'c-1'\", \"'a9'\", \"'g6'\", \"'c-1'\", \"'a9'\", \"'b6'\", \"'c-1'\", \"'a9'\", \"'d7'\", \"'c-1'\", \"'a9'\", \"'f7'\", \"'c-1'\", \"'a9'\", \"'g7'\", \"'c-1'\", \"'a9'\", \"'b7'\", \"'c-1'\", \"'c11'\", \"'g4'\", \"'g♯-1'\", \"'c11'\", \"'g5'\", \"'g♯0'\", \"'c11'\", \"'d6'\", \"'c0'\", \"'c11'\", \"'g6'\", \"'c1'\", \"'c11'\", \"'b6'\", \"'g♯-1'\", \"'c11'\", \"'d7'\", \"'e-1'\", \"'c11'\", \"'f7'\", \"'d-1'\", \"'c11'\", \"'g7'\", \"'f-1'\", \"'c11'\", \"'b7'\", \"'e-1'\", \"'c11'\", \"'d8'\", \"'d-1'\", \"'c♯11'\", \"'g4'\", \"'g♯-1'\", \"'c♯11'\", \"'g5'\", \"'g♯0'\", \"'c♯11'\", \"'d6'\", \"'c0'\", \"'c♯11'\", \"'g6'\", \"'c1'\", \"'c♯11'\", \"'b6'\", \"'g♯-1'\", \"'c♯11'\", \"'d7'\", \"'e-1'\", \"'c♯11'\", \"'f7'\", \"'d-1'\", \"'c♯11'\", \"'g7'\", \"'f-1'\", \"'c♯11'\", \"'b7'\", \"'e-1'\", \"'c♯11'\", \"'d8'\", \"'d-1'\", \"'g♯9'\", \"'g4'\", \"'c-1'\", \"'g♯9'\", \"'g5'\", \"'c-1'\", \"'g♯9'\", \"'d6'\", \"'c-1'\"]\n",
            "Guitar = [\"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'A-1'\", \"'C11'\", \"'G5'\", \"'E-1'\", \"'C11'\", \"'D6'\", \"'E-1'\", \"'C♯11'\", \"'G4'\", \"'A-1'\", \"'C♯11'\", \"'G5'\", \"'E-1'\", \"'C♯11'\", \"'D6'\", \"'E-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'G♯-1'\", \"'C11'\", \"'G5'\", \"'D-1'\", \"'C11'\", \"'D6'\", \"'E-1'\", \"'C♯11'\", \"'G4'\", \"'G♯-1'\", \"'C♯11'\", \"'G5'\", \"'D-1'\", \"'C♯11'\", \"'D6'\", \"'E-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C11'\", \"'G5'\", \"'D-1'\", \"'C11'\", \"'D6'\", \"'E-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'G5'\", \"'D-1'\", \"'C♯11'\", \"'D6'\", \"'E-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'C0'\", \"'C11'\", \"'G5'\", \"'E-1'\", \"'C11'\", \"'D6'\", \"'E-1'\", \"'C11'\", \"'G6'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'C0'\", \"'C♯11'\", \"'G5'\", \"'E-1'\", \"'C♯11'\", \"'D6'\", \"'E-1'\", \"'C♯11'\", \"'G6'\", \"'D-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'G6'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'G6'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D♯-1'\", \"'C11'\", \"'G5'\", \"'E-1'\", \"'C11'\", \"'D6'\", \"'F♯-1'\", \"'C11'\", \"'G6'\", \"'D♯-1'\", \"'C♯11'\", \"'G4'\", \"'D♯-1'\", \"'C♯11'\", \"'G5'\", \"'E-1'\", \"'C♯11'\", \"'D6'\", \"'F♯-1'\", \"'C♯11'\", \"'G6'\", \"'D♯-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'G6'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'G6'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'D-1'\", \"'C11'\", \"'G5'\", \"'D♯-1'\", \"'C11'\", \"'D6'\", \"'E-1'\", \"'C11'\", \"'G6'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'D-1'\", \"'C♯11'\", \"'G5'\", \"'D♯-1'\", \"'C♯11'\", \"'D6'\", \"'E-1'\", \"'C♯11'\", \"'G6'\", \"'D-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'G6'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G5'\", \"'C-1'\", \"'A9'\", \"'G6'\", \"'C-1'\", \"'A9'\", \"'D6'\", \"'C-1'\", \"'C11'\", \"'G4'\", \"'G-1'\", \"'C11'\", \"'G5'\", \"'F-1'\", \"'C11'\", \"'D6'\", \"'F-1'\", \"'C11'\", \"'G6'\", \"'D-1'\", \"'C♯11'\", \"'G4'\", \"'G-1'\", \"'C♯11'\", \"'G5'\", \"'F-1'\", \"'C♯11'\", \"'D6'\", \"'F-1'\", \"'C♯11'\", \"'G6'\", \"'D-1'\", \"'G♯9'\", \"'G4'\", \"'C-1'\", \"'G♯9'\", \"'G5'\", \"'C-1'\", \"'G♯9'\", \"'G6'\", \"'C-1'\", \"'G♯9'\", \"'D6'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted Guitar Note = {translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szBvYhwxE2CJ",
        "outputId": "6c076662-ef7b-4e3a-86a3-937b3cb93af8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', \"'C11'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\"]\n",
            "predicted Guitar Note = [\"'C11'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\", \"'C-1'\", \"'A9'\", \"'G4'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GKVxuI45FVS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}