{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMLFSIK0xEoG",
        "outputId": "a7ebf0b0-0d5a-4058-e893-64bd942f478f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 21.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.10.0.2)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WeZvkLV7xvMK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-6OUnWOjxw9a"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tENSm1HfyGvB"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/DataSetVoice/' # news_data or dict_data\n",
        "\n",
        "def tokenize(word): # create a tokenizer function\n",
        "    # word = word.replace('\\n', '')\n",
        "    return word.split(' ')\n",
        "\n",
        "# <sos>: start of a sequence; <eos>: end of a sequence.\n",
        "SRC = Field(tokenize=tokenize, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=True,\n",
        "            include_lengths = True)\n",
        "\n",
        "TRG = Field(tokenize=tokenize, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            lower=False,\n",
        "            include_lengths = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9G0vV4tZzSnR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_path+'sampled_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ-MNVIqznv8",
        "outputId": "80dc3fc2-af0d-4a9e-f2a9-287ecd5fd4ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(412000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NbfdHSTM1h66",
        "outputId": "9b2e0aca-efb1-48d3-9cdb-64ba89ef2d91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7306f241-559d-49f3-b075-ae66a0b239d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Song</th>\n",
              "      <th>Guitar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C11 G4 D♯-1</td>\n",
              "      <td>C11 D9 D-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C♯11 B6 D-1</td>\n",
              "      <td>C♯11 G4 D♯-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C11 D6 E-1</td>\n",
              "      <td>C11 D6 D♯-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C♯11 B6 D-1</td>\n",
              "      <td>G♯9 G9 C-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C♯11 B6 E-1</td>\n",
              "      <td>C11 C9 D♯-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411995</th>\n",
              "      <td>C♯11 D6 D♯-1</td>\n",
              "      <td>G♯9 G4 C-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411996</th>\n",
              "      <td>G♯9 G4 C-1</td>\n",
              "      <td>A9 G5 C-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411997</th>\n",
              "      <td>C♯11 G4 E-1</td>\n",
              "      <td>C♯11 G4 D-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411998</th>\n",
              "      <td>A9 G4 C-1</td>\n",
              "      <td>G♯9 G4 C-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411999</th>\n",
              "      <td>D♯20 B2 C-1</td>\n",
              "      <td>D♯20 B2 C-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>412000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7306f241-559d-49f3-b075-ae66a0b239d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7306f241-559d-49f3-b075-ae66a0b239d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7306f241-559d-49f3-b075-ae66a0b239d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Song        Guitar\n",
              "0        C11 G4 D♯-1    C11 D9 D-1\n",
              "1        C♯11 B6 D-1  C♯11 G4 D♯-1\n",
              "2         C11 D6 E-1   C11 D6 D♯-1\n",
              "3        C♯11 B6 D-1    G♯9 G9 C-1\n",
              "4        C♯11 B6 E-1   C11 C9 D♯-1\n",
              "...              ...           ...\n",
              "411995  C♯11 D6 D♯-1    G♯9 G4 C-1\n",
              "411996    G♯9 G4 C-1     A9 G5 C-1\n",
              "411997   C♯11 G4 E-1   C♯11 G4 D-1\n",
              "411998     A9 G4 C-1    G♯9 G4 C-1\n",
              "411999   D♯20 B2 C-1   D♯20 B2 C-1\n",
              "\n",
              "[412000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "APA4Q4ywzhpd"
      },
      "outputs": [],
      "source": [
        "train = df.iloc[0:400000]\n",
        "valid = df.iloc[400000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fXUn73Eizwht"
      },
      "outputs": [],
      "source": [
        "train.to_csv(\"train.csv\",index=False)\n",
        "valid.to_csv(\"valid.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lM-Vne6PzEDu"
      },
      "outputs": [],
      "source": [
        "train_data = TabularDataset(\n",
        "           path = \"train.csv\", \n",
        "           format='csv',\n",
        "           skip_header=True, \n",
        "           fields=([(\"Song\", SRC), (\"Guitar\", TRG)]))\n",
        "\n",
        "valid_data = TabularDataset(\n",
        "           path = \"valid.csv\", \n",
        "           format='csv',\n",
        "           skip_header=True, \n",
        "           fields=([(\"Song\", SRC), (\"Guitar\", TRG)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NlT2wFNh0GiE"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq=1)\n",
        "TRG.build_vocab(train_data, min_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94VEgBwY0SL5",
        "outputId": "551b89f5-f8ee-4614-9aaa-ba7125ac8891"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "133"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(SRC.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JyOGxgJ0V08",
        "outputId": "aac6167e-c23e-4e2f-8c1b-5e50414c0bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(TRG.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrzW1FB30ZiN",
        "outputId": "e8aeb394-c9d4-40b9-99e7-3a2ace50a9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 400000\n",
            "Number of testing examples: 12000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(valid_data.examples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O80m0sgY0kv8",
        "outputId": "02b11845-15db-4b9b-f645-442442608359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Song': ['a9', 'g4', 'c-1'], 'Guitar': ['C11', 'D6', 'D-1']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[20]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiQ52zvI13rL",
        "outputId": "d3480b4f-0859-405d-9c92-1776b89972de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = False,\n",
        "     sort_key = lambda x : len(x.Song),\n",
        "     device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6Ltsu2XK19it"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)    \n",
        "    def forward(self, src, src_len):        \n",
        "        embedded = self.dropout(self.embedding(src))                \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len,enforce_sorted=False)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jmNP3Pv-2AV6"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        return F.softmax(attention, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IUDBddN12DJw"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # print(output.shape)\n",
        "        # print(hidden.shape)\n",
        "        # assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jiLElypA2HEX"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device, teacher_forcing_ratio = 0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio \n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg,):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len.cpu())\n",
        "        input = trg[0,:]\n",
        "\n",
        "        mask = self.create_mask(src)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
        "            top1 = output.argmax(1) \n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zha16G3P2Iv9"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3clH86SK2KwR",
        "outputId": "19b3c1a6-1c4d-4bc6-c772-213d69c64834"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(133, 128)\n",
              "    (rnn): GRU(128, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(126, 128)\n",
              "    (rnn): GRU(640, 256)\n",
              "    (fc_out): Linear(in_features=896, out_features=126, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnMHTw_L2M8_",
        "outputId": "71580911-9218-4c28-b754-a5a7d2755830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,757,182 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W5d9guKL2Puq"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.003  # 0.003 in paper\n",
        "patience = 0\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer=optimizer,\n",
        "                mode='min', factor=0.9, # 0.9 in paper\n",
        "                patience=patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WM4Lpp552TF7"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wwugrPyJ2ax_"
      },
      "outputs": [],
      "source": [
        "def update(epoch, valid_loss, valid_acc, \n",
        "           best_valid_loss, best_valid_acc, acc_valid_loss,\n",
        "           update_type='acc'):\n",
        "    global best_valid_epoch, early_stop_patience, full_patience, best_train_step, train_steps, exp_num\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"[Epoch: {}][Validatiing...]\".format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        print('\\t\\t Better Valid Loss!')\n",
        "        best_valid_loss = valid_loss\n",
        "        if update_type == 'loss':\n",
        "            torch.save(model.state_dict(), 'loss-model.pt')\n",
        "        early_stop_patience = full_patience  # restore full patience if obtain new minimum of the loss\n",
        "    else:\n",
        "        if early_stop_patience > 0:\n",
        "            early_stop_patience += -1\n",
        "    \n",
        "    if valid_acc > best_valid_acc or (valid_acc == best_valid_acc and valid_loss < acc_valid_loss):\n",
        "        print('\\t\\t Better Valid Acc!')\n",
        "        best_valid_acc = valid_acc\n",
        "        acc_valid_loss = valid_loss\n",
        "        best_valid_epoch = epoch\n",
        "        best_train_step = train_steps\n",
        "        if update_type == 'acc':\n",
        "            torch.save(model.state_dict(), 'experiments/exp' + str(exp_num) + '/acc-model-seq2seq.pt')\n",
        "    print(f'\\t patience: {early_stop_patience}/{full_patience}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    print(f'\\t BEST. Val. Loss: {best_valid_loss:.3f} | BEST. Val. Acc: {best_valid_acc:.3f} | Val. Loss: {acc_valid_loss:.3f} | BEST. Val. Epoch: {best_valid_epoch} | BEST. Val. Step: {best_train_step}')\n",
        "    print(\"---------------------------------------\\n\")\n",
        "    return best_valid_loss, best_valid_acc, acc_valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uEQptv5E2iNk"
      },
      "outputs": [],
      "source": [
        "n_examples = len(train_data.examples)\n",
        "\n",
        "def train(model, iterator, \n",
        "          optimizer, criterion, \n",
        "          clip, epoch,\n",
        "          scheduler, valid_iterator):\n",
        "    \n",
        "    global best_valid_loss, acc_valid_loss, best_valid_acc, best_valid_epoch, train_steps, report_steps, tfr\n",
        "    model.train()\n",
        "    model.teacher_forcing_ratio = tfr\n",
        "    print(\"[Train]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.Song\n",
        "        trg, trg_len = batch.Guitar\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        running_loss = epoch_loss / (i + 1)\n",
        "        \n",
        "        # print every 50 batches (50 steps)\n",
        "        if i % report_steps == report_steps - 1:\n",
        "            train_steps += report_steps  # by doing so, the last batch is neglected\n",
        "            for param_group in optimizer.param_groups:\n",
        "                lr = param_group['lr']\n",
        "            print('[Epoch: {}][#examples: {}/{}][#steps: {}]'.format(epoch, (i+1) * BATCH_SIZE, n_examples, train_steps))\n",
        "            print(f'\\tTrain Loss: {running_loss:.3f} | Train PPL: {math.exp(running_loss):7.3f} | lr: {lr:.3e}')\n",
        "            \n",
        "            # eval the validation set for every * steps\n",
        "            if (train_steps % (10 * report_steps)) == 0:\n",
        "                print('-----val------')\n",
        "                valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler)\n",
        "                print('-----tst------')\n",
        "                # test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "                best_valid_loss, best_valid_acc, acc_valid_loss = update(epoch, valid_loss, valid_acc, \n",
        "                                                         best_valid_loss, best_valid_acc, acc_valid_loss,\n",
        "                                                         update_type='loss')\n",
        "                scheduler.step(valid_loss)  # must be placed here otherwise the test acc messes up\n",
        "                model.train()\n",
        "                \n",
        "            \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JTvJHDWE2j22"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, scheduler, is_test=False):\n",
        "    \n",
        "    model.eval()\n",
        "    model.teacher_forcing_ratio = 0 #  turn off teacher forcing\n",
        "    print(\"[Eval Start]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    global valid_data, test_data, tfr\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.Song\n",
        "            trg, trg_len = batch.Guitar\n",
        "\n",
        "            output = model(src, src_len, trg)\n",
        "            \n",
        "            # ---------compute acc START----------\n",
        "            pred = output[1:].argmax(2).permute(1, 0) # [batch_size, trg_len]\n",
        "            ref = trg[1:].permute(1, 0)\n",
        "            # consider the last batch as well\n",
        "            size = pred.shape[0]\n",
        "            for j in range(size):\n",
        "                \n",
        "                pred_j = pred[j, :]\n",
        "                pred_j_toks = []\n",
        "                for t in pred_j:\n",
        "                    tok = TRG.vocab.itos[t]\n",
        "                    if tok == '<eos>':\n",
        "                        break\n",
        "                    else:\n",
        "                        pred_j_toks.append(tok)\n",
        "                pred_j = ''.join(pred_j_toks)\n",
        "                \n",
        "                ref_j = ref[j, :]\n",
        "                ref_j_toks = []\n",
        "                for t in ref_j:\n",
        "                    tok = TRG.vocab.itos[t]\n",
        "                    if tok == '<eos>':\n",
        "                        break\n",
        "                    else:\n",
        "                        ref_j_toks.append(tok)\n",
        "                ref_j = ''.join(ref_j_toks)\n",
        "                \n",
        "                if pred_j == ref_j:\n",
        "                    correct += 1\n",
        "            # ---------compute acc END----------\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        # compute loss and acc\n",
        "        epoch_loss = epoch_loss / len(iterator)\n",
        "        # sheduler applies on acc\n",
        "        if not is_test:\n",
        "            acc = correct / len(valid_data.examples)\n",
        "            \n",
        "        else:\n",
        "            acc = correct / len(test_data.examples)\n",
        "        \n",
        "        print('The number of correct predictions: {}'.format(correct))\n",
        "        \n",
        "        model.teacher_forcing_ratio = tfr  # restore teacher-forcing ratio\n",
        "        print(\"[Eval End]: Current Teacher Forcing Ratio: {:.3f}\".format(model.teacher_forcing_ratio))\n",
        "    \n",
        "    return epoch_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GG8sqkiQ2mo9"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H-yBw8h2oXj",
        "outputId": "4c6a0ba5-ba65-4496-aab9-cc7289f42d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train]: Current Teacher Forcing Ratio: 0.800\n",
            "[Epoch: 0][#examples: 1280/400000][#steps: 10]\n",
            "\tTrain Loss: 3.749 | Train PPL:  42.489 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 2560/400000][#steps: 20]\n",
            "\tTrain Loss: 3.037 | Train PPL:  20.841 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 3840/400000][#steps: 30]\n",
            "\tTrain Loss: 2.612 | Train PPL:  13.630 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 5120/400000][#steps: 40]\n",
            "\tTrain Loss: 2.329 | Train PPL:  10.267 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 6400/400000][#steps: 50]\n",
            "\tTrain Loss: 2.159 | Train PPL:   8.666 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 7680/400000][#steps: 60]\n",
            "\tTrain Loss: 2.039 | Train PPL:   7.682 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 8960/400000][#steps: 70]\n",
            "\tTrain Loss: 1.951 | Train PPL:   7.035 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 10240/400000][#steps: 80]\n",
            "\tTrain Loss: 1.883 | Train PPL:   6.574 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 11520/400000][#steps: 90]\n",
            "\tTrain Loss: 1.830 | Train PPL:   6.232 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 12800/400000][#steps: 100]\n",
            "\tTrain Loss: 1.788 | Train PPL:   5.978 | lr: 3.000e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1239\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Loss!\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 20/20\n",
            "\t Val. Loss: 1.284 | Val. Acc: 0.103 | Val. PPL:   3.612\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.103 | Val. Loss: 1.284 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 14080/400000][#steps: 110]\n",
            "\tTrain Loss: 1.753 | Train PPL:   5.771 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 15360/400000][#steps: 120]\n",
            "\tTrain Loss: 1.721 | Train PPL:   5.588 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 16640/400000][#steps: 130]\n",
            "\tTrain Loss: 1.692 | Train PPL:   5.430 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 17920/400000][#steps: 140]\n",
            "\tTrain Loss: 1.666 | Train PPL:   5.293 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 19200/400000][#steps: 150]\n",
            "\tTrain Loss: 1.648 | Train PPL:   5.197 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 20480/400000][#steps: 160]\n",
            "\tTrain Loss: 1.639 | Train PPL:   5.148 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 21760/400000][#steps: 170]\n",
            "\tTrain Loss: 1.621 | Train PPL:   5.058 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 23040/400000][#steps: 180]\n",
            "\tTrain Loss: 1.604 | Train PPL:   4.972 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 24320/400000][#steps: 190]\n",
            "\tTrain Loss: 1.590 | Train PPL:   4.903 | lr: 3.000e-03\n",
            "[Epoch: 0][#examples: 25600/400000][#steps: 200]\n",
            "\tTrain Loss: 1.577 | Train PPL:   4.839 | lr: 3.000e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1239\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 19/20\n",
            "\t Val. Loss: 1.297 | Val. Acc: 0.103 | Val. PPL:   3.657\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.103 | Val. Loss: 1.284 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 26880/400000][#steps: 210]\n",
            "\tTrain Loss: 1.568 | Train PPL:   4.797 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 28160/400000][#steps: 220]\n",
            "\tTrain Loss: 1.557 | Train PPL:   4.745 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 29440/400000][#steps: 230]\n",
            "\tTrain Loss: 1.547 | Train PPL:   4.697 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 30720/400000][#steps: 240]\n",
            "\tTrain Loss: 1.540 | Train PPL:   4.662 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 32000/400000][#steps: 250]\n",
            "\tTrain Loss: 1.532 | Train PPL:   4.627 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 33280/400000][#steps: 260]\n",
            "\tTrain Loss: 1.526 | Train PPL:   4.599 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 34560/400000][#steps: 270]\n",
            "\tTrain Loss: 1.518 | Train PPL:   4.561 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 35840/400000][#steps: 280]\n",
            "\tTrain Loss: 1.511 | Train PPL:   4.530 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 37120/400000][#steps: 290]\n",
            "\tTrain Loss: 1.504 | Train PPL:   4.499 | lr: 2.700e-03\n",
            "[Epoch: 0][#examples: 38400/400000][#steps: 300]\n",
            "\tTrain Loss: 1.500 | Train PPL:   4.480 | lr: 2.700e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 792\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 18/20\n",
            "\t Val. Loss: 1.431 | Val. Acc: 0.066 | Val. PPL:   4.182\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.103 | Val. Loss: 1.284 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 39680/400000][#steps: 310]\n",
            "\tTrain Loss: 1.493 | Train PPL:   4.451 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 40960/400000][#steps: 320]\n",
            "\tTrain Loss: 1.485 | Train PPL:   4.417 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 42240/400000][#steps: 330]\n",
            "\tTrain Loss: 1.483 | Train PPL:   4.405 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 43520/400000][#steps: 340]\n",
            "\tTrain Loss: 1.476 | Train PPL:   4.376 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 44800/400000][#steps: 350]\n",
            "\tTrain Loss: 1.472 | Train PPL:   4.357 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 46080/400000][#steps: 360]\n",
            "\tTrain Loss: 1.466 | Train PPL:   4.332 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 47360/400000][#steps: 370]\n",
            "\tTrain Loss: 1.461 | Train PPL:   4.309 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 48640/400000][#steps: 380]\n",
            "\tTrain Loss: 1.458 | Train PPL:   4.297 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 49920/400000][#steps: 390]\n",
            "\tTrain Loss: 1.454 | Train PPL:   4.278 | lr: 2.430e-03\n",
            "[Epoch: 0][#examples: 51200/400000][#steps: 400]\n",
            "\tTrain Loss: 1.450 | Train PPL:   4.264 | lr: 2.430e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1234\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 17/20\n",
            "\t Val. Loss: 1.541 | Val. Acc: 0.103 | Val. PPL:   4.671\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.103 | Val. Loss: 1.284 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 52480/400000][#steps: 410]\n",
            "\tTrain Loss: 1.447 | Train PPL:   4.250 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 53760/400000][#steps: 420]\n",
            "\tTrain Loss: 1.445 | Train PPL:   4.243 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 55040/400000][#steps: 430]\n",
            "\tTrain Loss: 1.442 | Train PPL:   4.231 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 56320/400000][#steps: 440]\n",
            "\tTrain Loss: 1.439 | Train PPL:   4.215 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 57600/400000][#steps: 450]\n",
            "\tTrain Loss: 1.436 | Train PPL:   4.206 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 58880/400000][#steps: 460]\n",
            "\tTrain Loss: 1.432 | Train PPL:   4.189 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 60160/400000][#steps: 470]\n",
            "\tTrain Loss: 1.431 | Train PPL:   4.182 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 61440/400000][#steps: 480]\n",
            "\tTrain Loss: 1.428 | Train PPL:   4.170 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 62720/400000][#steps: 490]\n",
            "\tTrain Loss: 1.425 | Train PPL:   4.158 | lr: 2.187e-03\n",
            "[Epoch: 0][#examples: 64000/400000][#steps: 500]\n",
            "\tTrain Loss: 1.423 | Train PPL:   4.151 | lr: 2.187e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 165\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 16/20\n",
            "\t Val. Loss: 1.368 | Val. Acc: 0.014 | Val. PPL:   3.927\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.103 | Val. Loss: 1.284 | BEST. Val. Epoch: 0 | BEST. Val. Step: 100\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 65280/400000][#steps: 510]\n",
            "\tTrain Loss: 1.420 | Train PPL:   4.137 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 66560/400000][#steps: 520]\n",
            "\tTrain Loss: 1.419 | Train PPL:   4.134 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 67840/400000][#steps: 530]\n",
            "\tTrain Loss: 1.416 | Train PPL:   4.121 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 69120/400000][#steps: 540]\n",
            "\tTrain Loss: 1.413 | Train PPL:   4.107 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 70400/400000][#steps: 550]\n",
            "\tTrain Loss: 1.411 | Train PPL:   4.100 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 71680/400000][#steps: 560]\n",
            "\tTrain Loss: 1.410 | Train PPL:   4.094 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 72960/400000][#steps: 570]\n",
            "\tTrain Loss: 1.407 | Train PPL:   4.084 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 74240/400000][#steps: 580]\n",
            "\tTrain Loss: 1.405 | Train PPL:   4.074 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 75520/400000][#steps: 590]\n",
            "\tTrain Loss: 1.402 | Train PPL:   4.065 | lr: 1.968e-03\n",
            "[Epoch: 0][#examples: 76800/400000][#steps: 600]\n",
            "\tTrain Loss: 1.401 | Train PPL:   4.060 | lr: 1.968e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1245\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 15/20\n",
            "\t Val. Loss: 1.569 | Val. Acc: 0.104 | Val. PPL:   4.803\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.104 | Val. Loss: 1.569 | BEST. Val. Epoch: 0 | BEST. Val. Step: 600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 78080/400000][#steps: 610]\n",
            "\tTrain Loss: 1.401 | Train PPL:   4.059 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 79360/400000][#steps: 620]\n",
            "\tTrain Loss: 1.400 | Train PPL:   4.056 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 80640/400000][#steps: 630]\n",
            "\tTrain Loss: 1.398 | Train PPL:   4.047 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 81920/400000][#steps: 640]\n",
            "\tTrain Loss: 1.398 | Train PPL:   4.047 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 83200/400000][#steps: 650]\n",
            "\tTrain Loss: 1.396 | Train PPL:   4.040 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 84480/400000][#steps: 660]\n",
            "\tTrain Loss: 1.395 | Train PPL:   4.033 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 85760/400000][#steps: 670]\n",
            "\tTrain Loss: 1.393 | Train PPL:   4.027 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 87040/400000][#steps: 680]\n",
            "\tTrain Loss: 1.391 | Train PPL:   4.019 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 88320/400000][#steps: 690]\n",
            "\tTrain Loss: 1.389 | Train PPL:   4.010 | lr: 1.771e-03\n",
            "[Epoch: 0][#examples: 89600/400000][#steps: 700]\n",
            "\tTrain Loss: 1.387 | Train PPL:   4.002 | lr: 1.771e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1245\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 14/20\n",
            "\t Val. Loss: 1.455 | Val. Acc: 0.104 | Val. PPL:   4.286\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 90880/400000][#steps: 710]\n",
            "\tTrain Loss: 1.386 | Train PPL:   4.001 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 92160/400000][#steps: 720]\n",
            "\tTrain Loss: 1.385 | Train PPL:   3.997 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 93440/400000][#steps: 730]\n",
            "\tTrain Loss: 1.384 | Train PPL:   3.989 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 94720/400000][#steps: 740]\n",
            "\tTrain Loss: 1.383 | Train PPL:   3.988 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 96000/400000][#steps: 750]\n",
            "\tTrain Loss: 1.382 | Train PPL:   3.983 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 97280/400000][#steps: 760]\n",
            "\tTrain Loss: 1.381 | Train PPL:   3.978 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 98560/400000][#steps: 770]\n",
            "\tTrain Loss: 1.379 | Train PPL:   3.972 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 99840/400000][#steps: 780]\n",
            "\tTrain Loss: 1.377 | Train PPL:   3.964 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 101120/400000][#steps: 790]\n",
            "\tTrain Loss: 1.376 | Train PPL:   3.957 | lr: 1.594e-03\n",
            "[Epoch: 0][#examples: 102400/400000][#steps: 800]\n",
            "\tTrain Loss: 1.374 | Train PPL:   3.951 | lr: 1.594e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 146\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 13/20\n",
            "\t Val. Loss: 1.416 | Val. Acc: 0.012 | Val. PPL:   4.120\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 103680/400000][#steps: 810]\n",
            "\tTrain Loss: 1.373 | Train PPL:   3.948 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 104960/400000][#steps: 820]\n",
            "\tTrain Loss: 1.372 | Train PPL:   3.942 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 106240/400000][#steps: 830]\n",
            "\tTrain Loss: 1.372 | Train PPL:   3.942 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 107520/400000][#steps: 840]\n",
            "\tTrain Loss: 1.371 | Train PPL:   3.939 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 108800/400000][#steps: 850]\n",
            "\tTrain Loss: 1.371 | Train PPL:   3.938 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 110080/400000][#steps: 860]\n",
            "\tTrain Loss: 1.369 | Train PPL:   3.932 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 111360/400000][#steps: 870]\n",
            "\tTrain Loss: 1.369 | Train PPL:   3.930 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 112640/400000][#steps: 880]\n",
            "\tTrain Loss: 1.368 | Train PPL:   3.926 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 113920/400000][#steps: 890]\n",
            "\tTrain Loss: 1.367 | Train PPL:   3.926 | lr: 1.435e-03\n",
            "[Epoch: 0][#examples: 115200/400000][#steps: 900]\n",
            "\tTrain Loss: 1.367 | Train PPL:   3.923 | lr: 1.435e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 157\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 12/20\n",
            "\t Val. Loss: 1.382 | Val. Acc: 0.013 | Val. PPL:   3.982\n",
            "\t BEST. Val. Loss: 1.284 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 116480/400000][#steps: 910]\n",
            "\tTrain Loss: 1.366 | Train PPL:   3.921 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 117760/400000][#steps: 920]\n",
            "\tTrain Loss: 1.365 | Train PPL:   3.917 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 119040/400000][#steps: 930]\n",
            "\tTrain Loss: 1.364 | Train PPL:   3.913 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 120320/400000][#steps: 940]\n",
            "\tTrain Loss: 1.364 | Train PPL:   3.910 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 121600/400000][#steps: 950]\n",
            "\tTrain Loss: 1.363 | Train PPL:   3.907 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 122880/400000][#steps: 960]\n",
            "\tTrain Loss: 1.362 | Train PPL:   3.905 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 124160/400000][#steps: 970]\n",
            "\tTrain Loss: 1.361 | Train PPL:   3.902 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 125440/400000][#steps: 980]\n",
            "\tTrain Loss: 1.361 | Train PPL:   3.901 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 126720/400000][#steps: 990]\n",
            "\tTrain Loss: 1.360 | Train PPL:   3.898 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 128000/400000][#steps: 1000]\n",
            "\tTrain Loss: 1.360 | Train PPL:   3.896 | lr: 1.291e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 3\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Loss!\n",
            "\t patience: 20/20\n",
            "\t Val. Loss: 1.273 | Val. Acc: 0.000 | Val. PPL:   3.571\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 129280/400000][#steps: 1010]\n",
            "\tTrain Loss: 1.359 | Train PPL:   3.891 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 130560/400000][#steps: 1020]\n",
            "\tTrain Loss: 1.358 | Train PPL:   3.887 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 131840/400000][#steps: 1030]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.885 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 133120/400000][#steps: 1040]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.883 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 134400/400000][#steps: 1050]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.884 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 135680/400000][#steps: 1060]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.884 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 136960/400000][#steps: 1070]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.884 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 138240/400000][#steps: 1080]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.885 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 139520/400000][#steps: 1090]\n",
            "\tTrain Loss: 1.357 | Train PPL:   3.884 | lr: 1.291e-03\n",
            "[Epoch: 0][#examples: 140800/400000][#steps: 1100]\n",
            "\tTrain Loss: 1.356 | Train PPL:   3.882 | lr: 1.291e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 7\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 19/20\n",
            "\t Val. Loss: 1.325 | Val. Acc: 0.001 | Val. PPL:   3.762\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 142080/400000][#steps: 1110]\n",
            "\tTrain Loss: 1.356 | Train PPL:   3.879 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 143360/400000][#steps: 1120]\n",
            "\tTrain Loss: 1.355 | Train PPL:   3.878 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 144640/400000][#steps: 1130]\n",
            "\tTrain Loss: 1.355 | Train PPL:   3.876 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 145920/400000][#steps: 1140]\n",
            "\tTrain Loss: 1.355 | Train PPL:   3.875 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 147200/400000][#steps: 1150]\n",
            "\tTrain Loss: 1.354 | Train PPL:   3.871 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 148480/400000][#steps: 1160]\n",
            "\tTrain Loss: 1.353 | Train PPL:   3.869 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 149760/400000][#steps: 1170]\n",
            "\tTrain Loss: 1.353 | Train PPL:   3.869 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 151040/400000][#steps: 1180]\n",
            "\tTrain Loss: 1.353 | Train PPL:   3.867 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 152320/400000][#steps: 1190]\n",
            "\tTrain Loss: 1.352 | Train PPL:   3.866 | lr: 1.162e-03\n",
            "[Epoch: 0][#examples: 153600/400000][#steps: 1200]\n",
            "\tTrain Loss: 1.351 | Train PPL:   3.863 | lr: 1.162e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1234\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 18/20\n",
            "\t Val. Loss: 1.429 | Val. Acc: 0.103 | Val. PPL:   4.175\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 154880/400000][#steps: 1210]\n",
            "\tTrain Loss: 1.351 | Train PPL:   3.861 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 156160/400000][#steps: 1220]\n",
            "\tTrain Loss: 1.351 | Train PPL:   3.860 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 157440/400000][#steps: 1230]\n",
            "\tTrain Loss: 1.350 | Train PPL:   3.857 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 158720/400000][#steps: 1240]\n",
            "\tTrain Loss: 1.350 | Train PPL:   3.856 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 160000/400000][#steps: 1250]\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.854 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 161280/400000][#steps: 1260]\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.855 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 162560/400000][#steps: 1270]\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.852 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 163840/400000][#steps: 1280]\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.852 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 165120/400000][#steps: 1290]\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.852 | lr: 1.046e-03\n",
            "[Epoch: 0][#examples: 166400/400000][#steps: 1300]\n",
            "\tTrain Loss: 1.348 | Train PPL:   3.850 | lr: 1.046e-03\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 167\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 17/20\n",
            "\t Val. Loss: 1.355 | Val. Acc: 0.014 | Val. PPL:   3.878\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 167680/400000][#steps: 1310]\n",
            "\tTrain Loss: 1.348 | Train PPL:   3.849 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 168960/400000][#steps: 1320]\n",
            "\tTrain Loss: 1.348 | Train PPL:   3.849 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 170240/400000][#steps: 1330]\n",
            "\tTrain Loss: 1.347 | Train PPL:   3.848 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 171520/400000][#steps: 1340]\n",
            "\tTrain Loss: 1.347 | Train PPL:   3.845 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 172800/400000][#steps: 1350]\n",
            "\tTrain Loss: 1.346 | Train PPL:   3.842 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 174080/400000][#steps: 1360]\n",
            "\tTrain Loss: 1.345 | Train PPL:   3.840 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 175360/400000][#steps: 1370]\n",
            "\tTrain Loss: 1.345 | Train PPL:   3.837 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 176640/400000][#steps: 1380]\n",
            "\tTrain Loss: 1.344 | Train PPL:   3.836 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 177920/400000][#steps: 1390]\n",
            "\tTrain Loss: 1.344 | Train PPL:   3.833 | lr: 9.414e-04\n",
            "[Epoch: 0][#examples: 179200/400000][#steps: 1400]\n",
            "\tTrain Loss: 1.343 | Train PPL:   3.830 | lr: 9.414e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1240\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 16/20\n",
            "\t Val. Loss: 1.344 | Val. Acc: 0.103 | Val. PPL:   3.834\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 180480/400000][#steps: 1410]\n",
            "\tTrain Loss: 1.342 | Train PPL:   3.828 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 181760/400000][#steps: 1420]\n",
            "\tTrain Loss: 1.342 | Train PPL:   3.828 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 183040/400000][#steps: 1430]\n",
            "\tTrain Loss: 1.342 | Train PPL:   3.826 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 184320/400000][#steps: 1440]\n",
            "\tTrain Loss: 1.342 | Train PPL:   3.825 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 185600/400000][#steps: 1450]\n",
            "\tTrain Loss: 1.341 | Train PPL:   3.823 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 186880/400000][#steps: 1460]\n",
            "\tTrain Loss: 1.341 | Train PPL:   3.823 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 188160/400000][#steps: 1470]\n",
            "\tTrain Loss: 1.341 | Train PPL:   3.824 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 189440/400000][#steps: 1480]\n",
            "\tTrain Loss: 1.341 | Train PPL:   3.823 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 190720/400000][#steps: 1490]\n",
            "\tTrain Loss: 1.341 | Train PPL:   3.823 | lr: 8.473e-04\n",
            "[Epoch: 0][#examples: 192000/400000][#steps: 1500]\n",
            "\tTrain Loss: 1.340 | Train PPL:   3.821 | lr: 8.473e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 839\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 15/20\n",
            "\t Val. Loss: 1.345 | Val. Acc: 0.070 | Val. PPL:   3.839\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 193280/400000][#steps: 1510]\n",
            "\tTrain Loss: 1.340 | Train PPL:   3.821 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 194560/400000][#steps: 1520]\n",
            "\tTrain Loss: 1.340 | Train PPL:   3.820 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 195840/400000][#steps: 1530]\n",
            "\tTrain Loss: 1.340 | Train PPL:   3.818 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 197120/400000][#steps: 1540]\n",
            "\tTrain Loss: 1.340 | Train PPL:   3.818 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 198400/400000][#steps: 1550]\n",
            "\tTrain Loss: 1.339 | Train PPL:   3.817 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 199680/400000][#steps: 1560]\n",
            "\tTrain Loss: 1.339 | Train PPL:   3.815 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 200960/400000][#steps: 1570]\n",
            "\tTrain Loss: 1.339 | Train PPL:   3.815 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 202240/400000][#steps: 1580]\n",
            "\tTrain Loss: 1.339 | Train PPL:   3.814 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 203520/400000][#steps: 1590]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.813 | lr: 7.626e-04\n",
            "[Epoch: 0][#examples: 204800/400000][#steps: 1600]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.813 | lr: 7.626e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1242\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 14/20\n",
            "\t Val. Loss: 1.363 | Val. Acc: 0.103 | Val. PPL:   3.907\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 206080/400000][#steps: 1610]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.813 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 207360/400000][#steps: 1620]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.811 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 208640/400000][#steps: 1630]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.811 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 209920/400000][#steps: 1640]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.812 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 211200/400000][#steps: 1650]\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.810 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 212480/400000][#steps: 1660]\n",
            "\tTrain Loss: 1.337 | Train PPL:   3.808 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 213760/400000][#steps: 1670]\n",
            "\tTrain Loss: 1.337 | Train PPL:   3.807 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 215040/400000][#steps: 1680]\n",
            "\tTrain Loss: 1.337 | Train PPL:   3.807 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 216320/400000][#steps: 1690]\n",
            "\tTrain Loss: 1.337 | Train PPL:   3.806 | lr: 6.863e-04\n",
            "[Epoch: 0][#examples: 217600/400000][#steps: 1700]\n",
            "\tTrain Loss: 1.337 | Train PPL:   3.806 | lr: 6.863e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 157\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 13/20\n",
            "\t Val. Loss: 1.376 | Val. Acc: 0.013 | Val. PPL:   3.958\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 218880/400000][#steps: 1710]\n",
            "\tTrain Loss: 1.336 | Train PPL:   3.804 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 220160/400000][#steps: 1720]\n",
            "\tTrain Loss: 1.336 | Train PPL:   3.803 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 221440/400000][#steps: 1730]\n",
            "\tTrain Loss: 1.335 | Train PPL:   3.801 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 222720/400000][#steps: 1740]\n",
            "\tTrain Loss: 1.335 | Train PPL:   3.800 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 224000/400000][#steps: 1750]\n",
            "\tTrain Loss: 1.335 | Train PPL:   3.799 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 225280/400000][#steps: 1760]\n",
            "\tTrain Loss: 1.334 | Train PPL:   3.798 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 226560/400000][#steps: 1770]\n",
            "\tTrain Loss: 1.334 | Train PPL:   3.797 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 227840/400000][#steps: 1780]\n",
            "\tTrain Loss: 1.334 | Train PPL:   3.796 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 229120/400000][#steps: 1790]\n",
            "\tTrain Loss: 1.334 | Train PPL:   3.796 | lr: 6.177e-04\n",
            "[Epoch: 0][#examples: 230400/400000][#steps: 1800]\n",
            "\tTrain Loss: 1.333 | Train PPL:   3.794 | lr: 6.177e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 157\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 12/20\n",
            "\t Val. Loss: 1.347 | Val. Acc: 0.013 | Val. PPL:   3.847\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.455 | BEST. Val. Epoch: 0 | BEST. Val. Step: 700\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 231680/400000][#steps: 1810]\n",
            "\tTrain Loss: 1.333 | Train PPL:   3.793 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 232960/400000][#steps: 1820]\n",
            "\tTrain Loss: 1.333 | Train PPL:   3.792 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 234240/400000][#steps: 1830]\n",
            "\tTrain Loss: 1.333 | Train PPL:   3.791 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 235520/400000][#steps: 1840]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.789 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 236800/400000][#steps: 1850]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.788 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 238080/400000][#steps: 1860]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.787 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 239360/400000][#steps: 1870]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.787 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 240640/400000][#steps: 1880]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.788 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 241920/400000][#steps: 1890]\n",
            "\tTrain Loss: 1.332 | Train PPL:   3.788 | lr: 5.559e-04\n",
            "[Epoch: 0][#examples: 243200/400000][#steps: 1900]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.786 | lr: 5.559e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1248\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 11/20\n",
            "\t Val. Loss: 1.382 | Val. Acc: 0.104 | Val. PPL:   3.983\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 244480/400000][#steps: 1910]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.785 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 245760/400000][#steps: 1920]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.784 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 247040/400000][#steps: 1930]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.784 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 248320/400000][#steps: 1940]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.784 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 249600/400000][#steps: 1950]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.784 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 250880/400000][#steps: 1960]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.783 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 252160/400000][#steps: 1970]\n",
            "\tTrain Loss: 1.331 | Train PPL:   3.783 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 253440/400000][#steps: 1980]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.783 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 254720/400000][#steps: 1990]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.782 | lr: 5.003e-04\n",
            "[Epoch: 0][#examples: 256000/400000][#steps: 2000]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.782 | lr: 5.003e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 141\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 10/20\n",
            "\t Val. Loss: 1.329 | Val. Acc: 0.012 | Val. PPL:   3.775\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 257280/400000][#steps: 2010]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.781 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 258560/400000][#steps: 2020]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.780 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 259840/400000][#steps: 2030]\n",
            "\tTrain Loss: 1.330 | Train PPL:   3.780 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 261120/400000][#steps: 2040]\n",
            "\tTrain Loss: 1.329 | Train PPL:   3.778 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 262400/400000][#steps: 2050]\n",
            "\tTrain Loss: 1.329 | Train PPL:   3.777 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 263680/400000][#steps: 2060]\n",
            "\tTrain Loss: 1.329 | Train PPL:   3.776 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 264960/400000][#steps: 2070]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.775 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 266240/400000][#steps: 2080]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.774 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 267520/400000][#steps: 2090]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.774 | lr: 4.503e-04\n",
            "[Epoch: 0][#examples: 268800/400000][#steps: 2100]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.772 | lr: 4.503e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1248\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 9/20\n",
            "\t Val. Loss: 1.405 | Val. Acc: 0.104 | Val. PPL:   4.076\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 270080/400000][#steps: 2110]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.772 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 271360/400000][#steps: 2120]\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.772 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 272640/400000][#steps: 2130]\n",
            "\tTrain Loss: 1.327 | Train PPL:   3.771 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 273920/400000][#steps: 2140]\n",
            "\tTrain Loss: 1.327 | Train PPL:   3.770 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 275200/400000][#steps: 2150]\n",
            "\tTrain Loss: 1.327 | Train PPL:   3.768 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 276480/400000][#steps: 2160]\n",
            "\tTrain Loss: 1.326 | Train PPL:   3.767 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 277760/400000][#steps: 2170]\n",
            "\tTrain Loss: 1.326 | Train PPL:   3.767 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 279040/400000][#steps: 2180]\n",
            "\tTrain Loss: 1.326 | Train PPL:   3.766 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 280320/400000][#steps: 2190]\n",
            "\tTrain Loss: 1.326 | Train PPL:   3.766 | lr: 4.053e-04\n",
            "[Epoch: 0][#examples: 281600/400000][#steps: 2200]\n",
            "\tTrain Loss: 1.326 | Train PPL:   3.765 | lr: 4.053e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1085\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 8/20\n",
            "\t Val. Loss: 1.413 | Val. Acc: 0.090 | Val. PPL:   4.107\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 282880/400000][#steps: 2210]\n",
            "\tTrain Loss: 1.325 | Train PPL:   3.764 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 284160/400000][#steps: 2220]\n",
            "\tTrain Loss: 1.325 | Train PPL:   3.763 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 285440/400000][#steps: 2230]\n",
            "\tTrain Loss: 1.325 | Train PPL:   3.762 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 286720/400000][#steps: 2240]\n",
            "\tTrain Loss: 1.325 | Train PPL:   3.762 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 288000/400000][#steps: 2250]\n",
            "\tTrain Loss: 1.325 | Train PPL:   3.761 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 289280/400000][#steps: 2260]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.760 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 290560/400000][#steps: 2270]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.760 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 291840/400000][#steps: 2280]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.758 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 293120/400000][#steps: 2290]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.758 | lr: 3.647e-04\n",
            "[Epoch: 0][#examples: 294400/400000][#steps: 2300]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.758 | lr: 3.647e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 155\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 7/20\n",
            "\t Val. Loss: 1.322 | Val. Acc: 0.013 | Val. PPL:   3.752\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 295680/400000][#steps: 2310]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.757 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 296960/400000][#steps: 2320]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.758 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 298240/400000][#steps: 2330]\n",
            "\tTrain Loss: 1.324 | Train PPL:   3.757 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 299520/400000][#steps: 2340]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.757 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 300800/400000][#steps: 2350]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.755 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 302080/400000][#steps: 2360]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.755 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 303360/400000][#steps: 2370]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 304640/400000][#steps: 2380]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.753 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 305920/400000][#steps: 2390]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.753 | lr: 3.283e-04\n",
            "[Epoch: 0][#examples: 307200/400000][#steps: 2400]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 3.283e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 162\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 6/20\n",
            "\t Val. Loss: 1.397 | Val. Acc: 0.013 | Val. PPL:   4.044\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 308480/400000][#steps: 2410]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.753 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 309760/400000][#steps: 2420]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 311040/400000][#steps: 2430]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 312320/400000][#steps: 2440]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 313600/400000][#steps: 2450]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.754 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 314880/400000][#steps: 2460]\n",
            "\tTrain Loss: 1.323 | Train PPL:   3.753 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 316160/400000][#steps: 2470]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.752 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 317440/400000][#steps: 2480]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.751 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 318720/400000][#steps: 2490]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.751 | lr: 2.954e-04\n",
            "[Epoch: 0][#examples: 320000/400000][#steps: 2500]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.752 | lr: 2.954e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1248\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 5/20\n",
            "\t Val. Loss: 1.447 | Val. Acc: 0.104 | Val. PPL:   4.251\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.382 | BEST. Val. Epoch: 0 | BEST. Val. Step: 1900\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 321280/400000][#steps: 2510]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.751 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 322560/400000][#steps: 2520]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.750 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 323840/400000][#steps: 2530]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.750 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 325120/400000][#steps: 2540]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.749 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 326400/400000][#steps: 2550]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.749 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 327680/400000][#steps: 2560]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.749 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 328960/400000][#steps: 2570]\n",
            "\tTrain Loss: 1.322 | Train PPL:   3.749 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 330240/400000][#steps: 2580]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.749 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 331520/400000][#steps: 2590]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.748 | lr: 2.659e-04\n",
            "[Epoch: 0][#examples: 332800/400000][#steps: 2600]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.748 | lr: 2.659e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1248\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t\t Better Valid Acc!\n",
            "\t patience: 4/20\n",
            "\t Val. Loss: 1.354 | Val. Acc: 0.104 | Val. PPL:   3.874\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 334080/400000][#steps: 2610]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.746 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 335360/400000][#steps: 2620]\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.746 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 336640/400000][#steps: 2630]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.745 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 337920/400000][#steps: 2640]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.744 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 339200/400000][#steps: 2650]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.743 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 340480/400000][#steps: 2660]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.743 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 341760/400000][#steps: 2670]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.742 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 343040/400000][#steps: 2680]\n",
            "\tTrain Loss: 1.320 | Train PPL:   3.742 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 344320/400000][#steps: 2690]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.741 | lr: 2.393e-04\n",
            "[Epoch: 0][#examples: 345600/400000][#steps: 2700]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.741 | lr: 2.393e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 157\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 3/20\n",
            "\t Val. Loss: 1.343 | Val. Acc: 0.013 | Val. PPL:   3.831\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 346880/400000][#steps: 2710]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.741 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 348160/400000][#steps: 2720]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.741 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 349440/400000][#steps: 2730]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.740 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 350720/400000][#steps: 2740]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.739 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 352000/400000][#steps: 2750]\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.738 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 353280/400000][#steps: 2760]\n",
            "\tTrain Loss: 1.318 | Train PPL:   3.737 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 354560/400000][#steps: 2770]\n",
            "\tTrain Loss: 1.318 | Train PPL:   3.736 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 355840/400000][#steps: 2780]\n",
            "\tTrain Loss: 1.318 | Train PPL:   3.736 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 357120/400000][#steps: 2790]\n",
            "\tTrain Loss: 1.318 | Train PPL:   3.735 | lr: 2.154e-04\n",
            "[Epoch: 0][#examples: 358400/400000][#steps: 2800]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.734 | lr: 2.154e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 166\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 2/20\n",
            "\t Val. Loss: 1.361 | Val. Acc: 0.014 | Val. PPL:   3.900\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 359680/400000][#steps: 2810]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.733 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 360960/400000][#steps: 2820]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.733 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 362240/400000][#steps: 2830]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 363520/400000][#steps: 2840]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 364800/400000][#steps: 2850]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.733 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 366080/400000][#steps: 2860]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 367360/400000][#steps: 2870]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 368640/400000][#steps: 2880]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 369920/400000][#steps: 2890]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "[Epoch: 0][#examples: 371200/400000][#steps: 2900]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.938e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1116\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 1/20\n",
            "\t Val. Loss: 1.417 | Val. Acc: 0.093 | Val. PPL:   4.123\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 372480/400000][#steps: 2910]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 373760/400000][#steps: 2920]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 375040/400000][#steps: 2930]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.732 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 376320/400000][#steps: 2940]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.731 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 377600/400000][#steps: 2950]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.731 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 378880/400000][#steps: 2960]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.731 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 380160/400000][#steps: 2970]\n",
            "\tTrain Loss: 1.317 | Train PPL:   3.731 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 381440/400000][#steps: 2980]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.730 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 382720/400000][#steps: 2990]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.729 | lr: 1.744e-04\n",
            "[Epoch: 0][#examples: 384000/400000][#steps: 3000]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.730 | lr: 1.744e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 166\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 0/20\n",
            "\t Val. Loss: 1.351 | Val. Acc: 0.014 | Val. PPL:   3.862\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 385280/400000][#steps: 3010]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.730 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 386560/400000][#steps: 3020]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.730 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 387840/400000][#steps: 3030]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.729 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 389120/400000][#steps: 3040]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.729 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 390400/400000][#steps: 3050]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.729 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 391680/400000][#steps: 3060]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.728 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 392960/400000][#steps: 3070]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.727 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 394240/400000][#steps: 3080]\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.727 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 395520/400000][#steps: 3090]\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.726 | lr: 1.570e-04\n",
            "[Epoch: 0][#examples: 396800/400000][#steps: 3100]\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.726 | lr: 1.570e-04\n",
            "-----val------\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 1248\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "-----tst------\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 0/20\n",
            "\t Val. Loss: 1.407 | Val. Acc: 0.104 | Val. PPL:   4.084\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "[Epoch: 0][#examples: 398080/400000][#steps: 3110]\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.725 | lr: 1.413e-04\n",
            "[Epoch: 0][#examples: 399360/400000][#steps: 3120]\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.724 | lr: 1.413e-04\n",
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 157\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "\n",
            "---------------------------------------\n",
            "[Epoch: 0][Validatiing...]\n",
            "\t patience: 0/20\n",
            "\t Val. Loss: 1.352 | Val. Acc: 0.013 | Val. PPL:   3.865\n",
            "\t BEST. Val. Loss: 1.273 | BEST. Val. Acc: 0.104 | Val. Loss: 1.354 | BEST. Val. Epoch: 0 | BEST. Val. Step: 2600\n",
            "---------------------------------------\n",
            "\n",
            "Epoch: 01 | Time: 2m 31s\n",
            "\tTrain Loss: 1.315 | Train PPL:   3.724\n",
            "\t Val. Loss: 1.352 | Val. Acc: 0.013 | Val. PPL:   3.865\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 1\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "acc_valid_loss = float('inf')\n",
        "best_valid_acc = float(-1)\n",
        "best_valid_epoch = -1\n",
        "best_train_step = -1\n",
        "full_patience = 20\n",
        "early_stop_patience = full_patience\n",
        "train_steps = 0\n",
        "report_steps = 10\n",
        "exp_num = 1\n",
        "\n",
        "try:\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        if epoch <= 15:\n",
        "            early_stop_patience = full_patience\n",
        "\n",
        "        if early_stop_patience == 0:\n",
        "            print(\"Early Stopping!\")\n",
        "            # break\n",
        "            # abandon early stopping because we found best epoch in a long run\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        tfr = max(1 - (float(10 + epoch * 1.5) / 50), 0.2) \n",
        "\n",
        "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP, epoch, scheduler, valid_iterator)\n",
        "\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler, is_test=False)\n",
        "        #test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        best_valid_loss, best_valid_acc, acc_valid_loss = update(epoch, valid_loss, valid_acc, \n",
        "                                                 best_valid_loss, best_valid_acc, acc_valid_loss, update_type='loss')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        # print(f'\\t Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test ACC: {test_acc:.3f}')\n",
        "except KeyboardInterrupt:\n",
        "        print(\"Exiting loop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8UmBClVIjGLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386f724b-d2fe-490b-dcb8-2132ec819cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Eval Start]: Current Teacher Forcing Ratio: 0.000\n",
            "The number of correct predictions: 3\n",
            "[Eval End]: Current Teacher Forcing Ratio: 0.800\n",
            "| Valid Loss: 1.273 | Valid PPL:   3.571 | Valid ACC: 0.000\n"
          ]
        }
      ],
      "source": [
        "exp_num = 1\n",
        "# model.load_state_dict(torch.load('experiments/exp' + str(exp_num) + '/acc-model-seq2seq.pt'))\n",
        "model.load_state_dict(torch.load('loss-model.pt'))\n",
        "\n",
        "\n",
        "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, scheduler, is_test=False)\n",
        "# test_loss, test_acc = evaluate(model, test_iterator, criterion, scheduler, is_test=True)\n",
        "\n",
        "# Note that the final translation accs might differ from below because of floating point error.\n",
        "# But they should be the same in most of the cases.\n",
        "print(f'| Valid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f} | Valid ACC: {valid_acc:.3f}')\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test ACC: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    print(trg_tokens)\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "metadata": {
        "id": "lUJOGGLs-KU4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 1\n",
        "\n",
        "src = vars(valid_data.examples[example_idx])['Song']\n",
        "trg = vars(valid_data.examples[example_idx])['Guitar']\n",
        "\n",
        "print(f'Song = {src}')\n",
        "print(f'Guitar = {trg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iokg_vQT-M6K",
        "outputId": "3c441717-9a91-4851-ca5f-3fd5df0405b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Song = ['c♯11', 'g4', 'e-1']\n",
            "Guitar = ['C11', 'G5', 'D♯-1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "# print(f'predicted Guitar Note = {translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szBvYhwxE2CJ",
        "outputId": "435721bc-9ab0-4a0c-a161-19cf1b97777c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'C11', 'G4', 'C-1', '<eos>']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}